{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e690f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import ot\n",
    "import ot.plot\n",
    "\n",
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "#@numba.jit # unfortunately this doesn't jit easily :(\n",
    "def tokenize(text):\n",
    "    # given a body of text, this splits into sentences, then processes each word in the sentence to remove\n",
    "    # non alphabetical characters... (? bad idea, what about users with numbers in their name)\n",
    "    # returns it as a list of lists of words, the format desired by gensims word2vec\n",
    "    \n",
    "    sentences = []\n",
    "    if type(text) == str:\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            processed = [regex.sub('', word.lower()) for word in sentence.split(' ') ]\n",
    "            processed = [word for word in processed if word not in set( ['' ])]\n",
    "            sentences.append(processed)\n",
    "    return sentences\n",
    "\n",
    "def average_vector(text, model):\n",
    "    present_keys = [x for x in text if x in model.wv.key_to_index ]\n",
    "    if not present_keys:\n",
    "        return np.array([0] * len( model.wv[ model.wv.index_to_key[0]]))\n",
    "    return sum( [model.wv[x] for x in present_keys] ) /len(present_keys)\n",
    "\n",
    "def average_vector_paragraph(text, model):\n",
    "    if text == []:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    return sum( average_vector(sentence, model)  for sentence in text )\n",
    "\n",
    "## Most similar posts?\n",
    "\n",
    "\n",
    "def similarity(vec_1, vec_2):\n",
    "    return sklearn.metrics.pairwise.cosine_similarity([vec_1], [vec_2])[0]\n",
    "\n",
    "def make_similarity_col(df, given_index):\n",
    "    given_vector = df['avg_vector'][given_index] \n",
    "    df['similarity'] = df['avg_vector'].apply( lambda x : similarity(x, given_vector))\n",
    "    \n",
    "# helper function for printing the most similar word vectors\n",
    "\n",
    "def sims(args, model):\n",
    "    for word, sim in model.wv.most_similar(**args, topn = 10):\n",
    "        print( f\"{word} - similarity {sim}\")    \n",
    "\n",
    "        \n",
    "        \n",
    "def train_w2v(tokenized_text):\n",
    "    # the train dataframe ot build the w2v model on\n",
    "    \n",
    "    corpus = []\n",
    "    for tokenized in tokenized_text:\n",
    "        corpus += tokenized\n",
    "\n",
    "    model = gensim.models.Word2Vec(sentences = corpus,  min_count=10, vector_size=300, epochs = 4)\n",
    "    #model_fasttext = gensim.models.FastText(sentences = corpus,  min_count=10, vector_size=200, epochs = 4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def vectorize(df, model):\n",
    "    df['avg_vector'] = df['tokenized_title'].apply(lambda text : average_vector_paragraph(text, model)) \n",
    "    X = np.vstack(df['avg_vector'].to_numpy())\n",
    "    #df.concat(axis = 1, X)\n",
    "    return X\n",
    "\n",
    "def unpack_vectors(text, model):\n",
    "    vectors = []\n",
    "    for sentance in text:\n",
    "        for word in sentance:\n",
    "            if word in model.wv.key_to_index.keys():\n",
    "                vectors.append(model.wv[word])\n",
    "    return np.asarray(vectors)\n",
    "\n",
    "def cloudify(df, model):\n",
    "    df['point_cloud'] = df['tokenized_title'].apply(lambda text : unpack_vectors(text, model)) \n",
    "\n",
    "    return df\n",
    "\n",
    "def ot_distance(cloud_a, cloud_b):\n",
    "    n_a = len(cloud_a)\n",
    "    n_b = len(cloud_b)\n",
    "    a, b = np.ones((n_a,)) / n_a, np.ones((n_b,)) / n_b \n",
    "    M = ot.dist(cloud_a, cloud_b)\n",
    "    M /= M.max()\n",
    "    d = ot.emd2(a, b, M)\n",
    "    return d\n",
    "\n",
    "def ot_distance_regularized(cloud_a, cloud_b):\n",
    "    n_a = len(cloud_a)\n",
    "    n_b = len(cloud_b)\n",
    "    a, b = np.ones((n_a,)) / n_a, np.ones((n_b,)) / n_b \n",
    "    M = ot.dist(cloud_a, cloud_b)\n",
    "    M /= M.max()\n",
    "    lambd = 1e-3\n",
    "    d = ot.sinkhorn2(a, b, M, lambd)[0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ae24b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/subreddit_WallStreetBets/otherdata/wsb_cleaned.csv\", nrows = None)\n",
    "\n",
    "df = df.dropna(subset = ['title','selftext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_title'] = df.title.apply(tokenize)\n",
    "df['tokenized_selftext'] = df.selftext.apply(tokenize)\n",
    "model = train_w2v(df['tokenized_title'].append(df['tokenized_selftext'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83256637",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.DataFrame(model.wv.key_to_index.keys())\n",
    "corpus_df['vector'] = corpus_df[0].apply(lambda x : model.wv[x])\n",
    "corpus_df.to_csv(\"learned_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4a617",
   "metadata": {},
   "source": [
    "## Cluster authors based on their word vector point cloud distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21419379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "310ac79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "author_counts = df.author.value_counts()\n",
    "\n",
    "frequent_poasters = list(author_counts [ author_counts > 5 ].index)\n",
    "author_counts [ author_counts > 5 ]\n",
    "\n",
    "cleaned = df[ df.author.isin(frequent_poasters)]\n",
    "\n",
    "compounded = cleaned[['tokenized_title', 'author']].groupby(\"author\").agg('sum')\n",
    "compounded.drop(labels = [\"None\", \"AutoModerator\"])\n",
    "clouded = cloudify(compounded, model)\n",
    "clouds = clouded[[point_cloud]]\n",
    "clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f9be92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "\n",
    "\n",
    "distances = np.zeros( shape= (len(clouds), len(clouds)) )\n",
    "# This is expensive, I don't wnat to redo it every time...\n",
    "for i in range(len(clouds)):\n",
    "    print(f\"Processing column {i} out of {len(clouds)}\")\n",
    "    for j in range(len(clouds)):\n",
    "        if i < j:\n",
    "            d = ot_distance_regularized(clouds.iloc[i], clouds.iloc[j])\n",
    "            distances[i,j] = d\n",
    "            distances[j,i] = d\n",
    "            \n",
    "aff_matrix = np.exp( -1 * distances / distances.std())\n",
    "\n",
    "sc = SpectralClustering(n_clusters = 8, affinity = 'precomputed')\n",
    "labels = sc.fit_predict(aff_matrix)\n",
    "clouds['clusters'] = labels\n",
    "clouds.clusters.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a31c793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a3974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0af3d374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65576ce0",
   "metadata": {},
   "source": [
    "It's hard to make sense of this because I don't know the users well enough to cluster them. \n",
    "\n",
    "If we cluster posts by title instead at least then the clusters can be evaluated by inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f404ef",
   "metadata": {},
   "source": [
    "## Cluster posts based on their word vector clouds.\n",
    "\n",
    "How well does nearest neighbor classification do?\n",
    "Can we build a Bayesian hierarchical model that takes into account any groups we find here?\n",
    "(If we find any conceptually meaningful clusters, what else can we do with that information?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc64363",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_clouded = cloudify(df, model)\n",
    "post_clouded = post_clouded [ post_clouded.point_cloud.apply(lambda x : len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = post_clouded[['id', 'title', 'point_cloud']][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fc690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distances = np.zeros( shape= (len(clouds), len(clouds)) )\n",
    "# This is expensive, I don't wnat to redo it every time...\n",
    "k = int(len(clouds) / 10)\n",
    "\n",
    "for i in range(len(clouds)):\n",
    "    if i % k == 0:\n",
    "        print(f\"Processing column {i} out of {len(clouds)}\")\n",
    "    for j in range(len(clouds)):\n",
    "        if i < j:\n",
    "            d = ot_distance(clouds.iloc[i].point_cloud, clouds.iloc[j].point_cloud)\n",
    "            distances[i,j] = d\n",
    "            distances[j,i] = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_matrix = np.exp( -1 * distances / distances.std())\n",
    "\n",
    "sc = SpectralClustering(n_clusters = int(len(clouds)/5), affinity = 'precomputed')\n",
    "labels = sc.fit_predict(aff_matrix)\n",
    "clouds['clusters'] = labels\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_rows = 100\n",
    "clouds[['id', 'title', 'clusters']].sort_values(by = 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47205240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
