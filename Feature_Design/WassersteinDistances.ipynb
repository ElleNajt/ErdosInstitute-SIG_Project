{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e690f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import ot\n",
    "import ot.plot\n",
    "\n",
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "import networkx as nx\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "\n",
    "#@numba.jit # unfortunately this doesn't jit easily :(\n",
    "def tokenize(text):\n",
    "    # given a body of text, this splits into sentences, then processes each word in the sentence to remove\n",
    "    # non alphabetical characters... (? bad idea, what about users with numbers in their name)\n",
    "    # returns it as a list of lists of words, the format desired by gensims word2vec\n",
    "    \n",
    "    sentences = []\n",
    "    if type(text) == str:\n",
    "        for sentence in nltk.tokenize.sent_tokenize(text):\n",
    "            processed = [regex.sub('', word.lower()) for word in sentence.split(' ') ]\n",
    "            processed = [word for word in processed if word not in set( ['' ])]\n",
    "            sentences.append(processed)\n",
    "    return sentences\n",
    "\n",
    "def average_vector(text, model):\n",
    "    present_keys = [x for x in text if x in model.wv.key_to_index ]\n",
    "    if not present_keys:\n",
    "        return np.array([0] * len( model.wv[ model.wv.index_to_key[0]]))\n",
    "    return sum( [model.wv[x] for x in present_keys] ) /len(present_keys)\n",
    "\n",
    "def average_vector_paragraph(text, model):\n",
    "    if text == []:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    return sum( average_vector(sentence, model)  for sentence in text )\n",
    "\n",
    "## Most similar posts?\n",
    "\n",
    "\n",
    "def similarity(vec_1, vec_2):\n",
    "    return sklearn.metrics.pairwise.cosine_similarity([vec_1], [vec_2])[0]\n",
    "\n",
    "def make_similarity_col(df, given_index):\n",
    "    given_vector = df['avg_vector'][given_index] \n",
    "    df['similarity'] = df['avg_vector'].apply( lambda x : similarity(x, given_vector))\n",
    "    \n",
    "# helper function for printing the most similar word vectors\n",
    "\n",
    "def sims(args, model):\n",
    "    for word, sim in model.wv.most_similar(**args, topn = 10):\n",
    "        print( f\"{word} - similarity {sim}\")    \n",
    "\n",
    "        \n",
    "        \n",
    "def train_w2v(tokenized_text):\n",
    "    # the train dataframe ot build the w2v model on\n",
    "    \n",
    "    corpus = []\n",
    "    for tokenized in tokenized_text:\n",
    "        corpus += tokenized\n",
    "\n",
    "    model = gensim.models.Word2Vec(sentences = corpus,  min_count=10, vector_size=300, epochs = 4)\n",
    "    #model_fasttext = gensim.models.FastText(sentences = corpus,  min_count=10, vector_size=200, epochs = 4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def vectorize(df, model):\n",
    "    df['avg_vector'] = df['tokenized_title'].apply(lambda text : average_vector_paragraph(text, model)) \n",
    "    X = np.vstack(df['avg_vector'].to_numpy())\n",
    "    #df.concat(axis = 1, X)\n",
    "    return X\n",
    "\n",
    "def unpack_vectors(text, model_dict, cut_off = 300):\n",
    "    vectors = []\n",
    "    \n",
    "    for sentance in text:\n",
    "        for word in sentance:\n",
    "            if word in model_dict.keys():\n",
    "                if word not in stopwords.words():\n",
    "                    vectors.append(model_dict[word])\n",
    "                if len(vectors) >= cut_off:\n",
    "                    return np.asarray(vectors)\n",
    "    return np.asarray(vectors)\n",
    "\n",
    "def cloudify(df, model_dict, cut_off = 300):\n",
    "    df['title_point_cloud'] = df['tokenized_title'].apply(lambda text : unpack_vectors(text, model_dict, cut_off)) \n",
    "    df['selftext_point_cloud']= df['tokenized_selftext'].apply(lambda text : unpack_vectors(text, model_dict, cut_off)) \n",
    "    df['merged_point_cloud'] = df.apply( lambda x : np.concatenate([x['title_point_cloud'], x['selftext_point_cloud']]), axis = 1 )\n",
    "    return df\n",
    "\n",
    "def uniform(length):\n",
    "    return np.ones( (length,))/ length\n",
    "\n",
    "def decay(length, step = .3, shift = 1):\n",
    "    #step and shift parameters try to mellow out the decay\n",
    "    likelihood = 1 / (np.cumsum(np.ones(length)*step) + shift)\n",
    "    return likelihood / sum(likelihood)\n",
    "\n",
    "def ot_distance(cloud_a, cloud_b, distribution = 'uniform'):\n",
    "    n_a = len(cloud_a)\n",
    "    n_b = len(cloud_b)\n",
    "    if distribution == 'uniform':\n",
    "        a, b = uniform(n_a), uniform(n_b)\n",
    "    if distribution == 'decay':\n",
    "        a, b = decay(n_a), decay(n_b)        \n",
    "    M = ot.dist(cloud_a, cloud_b)\n",
    "    M /= M.max()\n",
    "    d = ot.emd2(a, b, M)\n",
    "    return d\n",
    "\n",
    "def ot_distance_regularized(cloud_a, cloud_b):\n",
    "    n_a = len(cloud_a)\n",
    "    n_b = len(cloud_b)\n",
    "    a, b = np.ones((n_a,)) / n_a, np.ones((n_b,)) / n_b \n",
    "    M = ot.dist(cloud_a, cloud_b)\n",
    "    M /= M.max()\n",
    "    lambd = 1e-3\n",
    "    d = ot.sinkhorn2(a, b, M, lambd)[0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b64e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89561f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae24b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/subreddit_WallStreetBets/otherdata/wsb_cleaned.csv\", nrows = 5000)\n",
    "df = df.dropna(subset = ['title','selftext'])\n",
    "#df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd3a9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_title'] = df.title.apply(tokenize)\n",
    "df['tokenized_selftext'] = df.selftext.apply(tokenize)\n",
    "#model = train_w2v(df['tokenized_title'].append(df['tokenized_selftext'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed3d23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corpus_df = pd.read_csv('learned_embedding.csv')\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def fix_encoding(string):\n",
    "    string = string.replace('\\n', '')\n",
    "    string = _RE_COMBINE_WHITESPACE.sub(\" \", string).strip()\n",
    "    string = string.replace(\" \", \",\")\n",
    "    string = string.replace(\"[,\", \"[\")\n",
    "    return eval(string)\n",
    "model_dict = dict(corpus_df.set_index('0')['vector'].apply(lambda x : fix_encoding(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "83256637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_df = pd.DataFrame(model.wv.key_to_index.keys())\n",
    "#corpus_df['vector'] = corpus_df[0].apply(lambda x : model.wv[x])\n",
    "#corpus_df.to_csv(\"learned_embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00375d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71854333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "436478c0",
   "metadata": {},
   "source": [
    "## Cluster authors based on their word vector point cloud distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21419379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "310ac79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "author_counts = df.author.value_counts()\n",
    "\n",
    "frequent_poasters = list(author_counts [ author_counts > 5 ].index)\n",
    "author_counts [ author_counts > 5 ]\n",
    "\n",
    "cleaned = df[ df.author.isin(frequent_poasters)]\n",
    "\n",
    "compounded = cleaned[['tokenized_title', 'author']].groupby(\"author\").agg('sum')\n",
    "compounded.drop(labels = [\"None\", \"AutoModerator\"])\n",
    "clouded = cloudify(compounded, model)\n",
    "clouds = clouded[[point_cloud]]\n",
    "clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f9be92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false \n",
    "\n",
    "\n",
    "distances = np.zeros( shape= (len(clouds), len(clouds)) )\n",
    "# This is expensive, I don't wnat to redo it every time...\n",
    "for i in range(len(clouds)):\n",
    "    print(f\"Processing column {i} out of {len(clouds)}\")\n",
    "    for j in range(len(clouds)):\n",
    "        if i < j:\n",
    "            d = ot_distance_regularized(clouds.iloc[i], clouds.iloc[j])\n",
    "            distances[i,j] = d\n",
    "            distances[j,i] = d\n",
    "            \n",
    "aff_matrix = np.exp( -1 * distances / distances.std())\n",
    "\n",
    "sc = SpectralClustering(n_clusters = 8, affinity = 'precomputed')\n",
    "labels = sc.fit_predict(aff_matrix)\n",
    "clouds['clusters'] = labels\n",
    "clouds.clusters.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06477da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a3974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0af3d374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dda0cbb",
   "metadata": {},
   "source": [
    "It's hard to make sense of this because I don't know the users well enough to cluster them. \n",
    "\n",
    "If we cluster posts by title instead at least then the clusters can be evaluated by inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7de82",
   "metadata": {},
   "source": [
    "## Cluster posts based on their word vector clouds.\n",
    "\n",
    "How well does nearest neighbor classification do?\n",
    "Can we build a Bayesian hierarchical model that takes into account any groups we find here?\n",
    "(If we find any conceptually meaningful clusters, what else can we do with that information?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5fc64363",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100\n",
    "cut_off = 20 # ONly take the first cut_off words...\n",
    "df_slice = pd.DataFrame(df.loc[:num_rows, :])\n",
    "post_clouded = cloudify(df_slice, model_dict, cut_off = cut_off)\n",
    "cloud_col = ['selftext_point_cloud','title_point_cloud', 'merged_point_cloud'][2]\n",
    "\n",
    "post_clouded = pd.DataFrame(post_clouded [ post_clouded[cloud_col].apply(lambda x : len(x) > 0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788080e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5ac4ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b2931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "468394c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column 0 out of 74\n",
      "Processing column 7 out of 74\n",
      "Processing column 14 out of 74\n",
      "Processing column 21 out of 74\n",
      "Processing column 28 out of 74\n",
      "Processing column 35 out of 74\n",
      "Processing column 42 out of 74\n",
      "Processing column 49 out of 74\n",
      "Processing column 56 out of 74\n",
      "Processing column 63 out of 74\n",
      "Processing column 70 out of 74\n"
     ]
    }
   ],
   "source": [
    "distances = np.zeros( shape= (len(post_clouded), len(post_clouded)) )\n",
    "distances_decay = np.zeros( shape= (len(post_clouded), len(post_clouded)) )\n",
    "\n",
    "k = int(len(post_clouded) / 10)\n",
    "\n",
    "for i in range(len(post_clouded)):\n",
    "    if i % k == 0:\n",
    "        print(f\"Processing column {i} out of {len(post_clouded)}\")\n",
    "    for j in range(len(post_clouded)):\n",
    "        if i < j:\n",
    "            d = ot_distance(post_clouded.iloc[i][cloud_col], post_clouded.iloc[j][cloud_col], distribution = \"uniform\")\n",
    "            distances[i,j] = d\n",
    "            distances[j,i] = d\n",
    "            d = ot_distance(post_clouded.iloc[i][cloud_col], post_clouded.iloc[j][cloud_col], distribution = \"decay\")\n",
    "            distances_decay[i,j] = d\n",
    "            distances_decay[j,i] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e2dbcb2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distance_matrix = [ distances, distances_decay][0]\n",
    "\n",
    "aff_matrix = np.exp( -1 * distance_matrix / distance_matrix.std())\n",
    "\n",
    "sc = SpectralClustering(n_clusters = int(len(post_clouded)/5), affinity = 'precomputed')\n",
    "labels = sc.fit_predict(aff_matrix)\n",
    "post_clouded['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f5d095a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ups</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>eifobk</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot inside tip AMD crashing</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>eijzxq</td>\n",
       "      <td>1</td>\n",
       "      <td>What do you think about CD Projekt?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>eio958</td>\n",
       "      <td>3</td>\n",
       "      <td>Is Robinhood a good platform for buying small amounts of stock for kids</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>eik56h</td>\n",
       "      <td>1</td>\n",
       "      <td>What do you think about CD Projekt?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>einvbk</td>\n",
       "      <td>44</td>\n",
       "      <td>Advanced portfolio funding strategy</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>einsgv</td>\n",
       "      <td>5</td>\n",
       "      <td>Teach me your ways - Shorting Dry Stock Vessels</td>\n",
       "      <td>I'm pretty new to investing, so obviously my only experience is 2-3 weeks on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>eikbad</td>\n",
       "      <td>1</td>\n",
       "      <td>What do you think about CD Projekt?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>eijht4</td>\n",
       "      <td>1</td>\n",
       "      <td>A WSB lurker's 2019 results; primarily via position trading beaten down biot...</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>einjt5</td>\n",
       "      <td>3</td>\n",
       "      <td>Index funds donâ€™t run same risk of assignment as stocks?</td>\n",
       "      <td>Warning: actual useful information \\nStudying up on iron butterflies and it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>einfso</td>\n",
       "      <td>4</td>\n",
       "      <td>Phase 1 trade deal charity bet</td>\n",
       "      <td>Tards:\\nA few days ago, in response to news that the Phase1 trade deal is sc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>eile0m</td>\n",
       "      <td>2</td>\n",
       "      <td>Aurora Cannabis (ACB) announces $100 billion partnership deal with Coca-Cola...</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>eifu11</td>\n",
       "      <td>15</td>\n",
       "      <td>DD: AZN / MRK Got Approval For New pancreatic cancer treatment</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eimo9e</td>\n",
       "      <td>1</td>\n",
       "      <td>Top Tech Stocks 2020 under $100</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>einh9z</td>\n",
       "      <td>2</td>\n",
       "      <td>Priced in? Chinaâ€™s central bank inject $115 billion into the financial syste...</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eio01g</td>\n",
       "      <td>1</td>\n",
       "      <td>Financial History</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>eikk0r</td>\n",
       "      <td>1</td>\n",
       "      <td>dO yOu ThiNk Im PlaYing ArOuNd?</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ein4nt</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks for the hangover heart attack this morning, E*Trade...</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>eimigb</td>\n",
       "      <td>0</td>\n",
       "      <td>Please help now please assist</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>eiftuc</td>\n",
       "      <td>9</td>\n",
       "      <td>Dumbest trades of 2019</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>eiiq38</td>\n",
       "      <td>7</td>\n",
       "      <td>I've concluded we're all autistic dumbasses.</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>eihyh9</td>\n",
       "      <td>0</td>\n",
       "      <td>Noob getting started....</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>eih880</td>\n",
       "      <td>1</td>\n",
       "      <td>[SERIOUS] So, I've seen this picture on one of the most upvoted posts. Can s...</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eipn7c</td>\n",
       "      <td>11</td>\n",
       "      <td>I will remember</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>eijlkn</td>\n",
       "      <td>50</td>\n",
       "      <td>Can I buy calls on my alarm?</td>\n",
       "      <td>So I left my alarm on this morning and I kept hitting snooze as I usually do...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>eij4ma</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh how I wish the market was open today ðŸ˜¥</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>eilej0</td>\n",
       "      <td>1</td>\n",
       "      <td>My Wife: Spy Calls, Tesla Puts...</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eipxm4</td>\n",
       "      <td>0</td>\n",
       "      <td>i wanna buy a call but i don't want to bet too much</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eipw3g</td>\n",
       "      <td>5</td>\n",
       "      <td>Buy INTU - DD</td>\n",
       "      <td>https://www.schaeffersresearch.com/content/analysis/2019/12/31/intu-options-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>eileei</td>\n",
       "      <td>407</td>\n",
       "      <td>How we can all be retarded millionaires by the end of 2020</td>\n",
       "      <td>Literally only buy calls\\nFucking ACB is trending down? Fuck it calls.\\nFuck...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>eilpv9</td>\n",
       "      <td>1</td>\n",
       "      <td>ACB gonna moon boys</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>einz9m</td>\n",
       "      <td>173</td>\n",
       "      <td>Markets are on the cusp of a correction and there's opportunity for mega ten...</td>\n",
       "      <td>I was going to tag this as technical, but since this sub misunderstands hate...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>eii46z</td>\n",
       "      <td>1</td>\n",
       "      <td>Taking some time today to remember the original autist</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>eiop2z</td>\n",
       "      <td>6</td>\n",
       "      <td>DD on pot stocks</td>\n",
       "      <td>First day of legal weed in Illinois, passed by a store on lunchtime and saw ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>eii4ys</td>\n",
       "      <td>1</td>\n",
       "      <td>Whoever was on here talking about capes being popular was onto something! Ch...</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>eiicnv</td>\n",
       "      <td>557</td>\n",
       "      <td>You people have ruined me</td>\n",
       "      <td>Every time I talk in public or write some message I start talking like a god...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>eig6iz</td>\n",
       "      <td>41</td>\n",
       "      <td>2019 in review.</td>\n",
       "      <td>2019 was a wild ride for this degenerate sub. I learned some lessons which t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eip6af</td>\n",
       "      <td>27</td>\n",
       "      <td>How many of you tism's are doing the UPRO/TMF mix?</td>\n",
       "      <td>I saw an older thread on it, curious who is still doing it? How have your re...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eipxnr</td>\n",
       "      <td>6</td>\n",
       "      <td>Good time to get on $BLUE</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>eim7za</td>\n",
       "      <td>1</td>\n",
       "      <td>FCEL Pull and Pray time?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>eifxax</td>\n",
       "      <td>13</td>\n",
       "      <td>Happy new decade and shit</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>eip6bx</td>\n",
       "      <td>3</td>\n",
       "      <td>New strat literally can't go tits up</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eioo45</td>\n",
       "      <td>1</td>\n",
       "      <td>New year, new baseline, new records. We can only go up!, to the moon!</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>eim1g2</td>\n",
       "      <td>86</td>\n",
       "      <td>Real reason Intel why failed to deliver 10nm for so long is because of feminism</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>eiogf6</td>\n",
       "      <td>1</td>\n",
       "      <td>Shitpost Long $LULU $BECKY</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>einqmq</td>\n",
       "      <td>1</td>\n",
       "      <td>Playing the long game, soon I'll be a millionaire.</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>eim0fn</td>\n",
       "      <td>1</td>\n",
       "      <td>How Long Until We See Rivian IPO?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>eiis10</td>\n",
       "      <td>57</td>\n",
       "      <td>So whatâ€™s the deal with â€˜analystsâ€™?</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>eihzxb</td>\n",
       "      <td>1</td>\n",
       "      <td>ACB, whatâ€™s up</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>eilorf</td>\n",
       "      <td>1</td>\n",
       "      <td>AIMT Timing</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>eik4ba</td>\n",
       "      <td>1</td>\n",
       "      <td>Government's cutðŸ‡ºðŸ‡¸</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>einx0a</td>\n",
       "      <td>5</td>\n",
       "      <td>PSN Parsons Company</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>eigfbf</td>\n",
       "      <td>1</td>\n",
       "      <td>MRO Breakout?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>eio4c2</td>\n",
       "      <td>0</td>\n",
       "      <td>STOCK SALE</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>eiglvd</td>\n",
       "      <td>1</td>\n",
       "      <td>Which one of you was this ? (Tesla)</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>eiguo4</td>\n",
       "      <td>1</td>\n",
       "      <td>How many of you are on the spectrum?</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>eiojmk</td>\n",
       "      <td>6</td>\n",
       "      <td>AIMT Discussion</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>eih9st</td>\n",
       "      <td>37</td>\n",
       "      <td>Daily Discussion Thread - January 01, 2020</td>\n",
       "      <td>Your daily trading discussion thread. Please keep the shitposting to a minim...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>einrlu</td>\n",
       "      <td>64</td>\n",
       "      <td>What Are Your Moves Tomorrow, January 02</td>\n",
       "      <td>Daily Discussion Thread - DD - YOLO - Discussion\\nWeekly Earnings Discussion...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>eilld6</td>\n",
       "      <td>2</td>\n",
       "      <td>Withhold taxes all year, it's cheaper than margin.</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eio7dn</td>\n",
       "      <td>1899</td>\n",
       "      <td>Day Trading, how I lost it all.</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ein5hs</td>\n",
       "      <td>4</td>\n",
       "      <td>Pattern Day Trader workaround</td>\n",
       "      <td>If you trade options in a non margin account, PDT wont apply. Option trades ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ein18r</td>\n",
       "      <td>0</td>\n",
       "      <td>Best stock to take advantage of undervalued commodities?</td>\n",
       "      <td>General consensus is that commodities are way undervalued right now. I've be...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>eil2hi</td>\n",
       "      <td>25</td>\n",
       "      <td>Friendly reminder futures markets open at 5PM CT/6PM ET</td>\n",
       "      <td>Hope you recovered enough from your hangover. Let's get ready for another bu...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>eilqxl</td>\n",
       "      <td>7</td>\n",
       "      <td>AIMT Timing</td>\n",
       "      <td>So we all know that AR101 is scheduled to be approved \"late January\" and we ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>eihjak</td>\n",
       "      <td>4194</td>\n",
       "      <td>2019 1st Annual Wallstreetbets Awards Winners</td>\n",
       "      <td>Welcome to the 1st Annual Wallstreetbets Awards!\\nHere we will celebrate som...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>eij248</td>\n",
       "      <td>39</td>\n",
       "      <td>How to trade like a professional stonks autist in 2020.</td>\n",
       "      <td>We all love dropping all our $BECKY bucks on a good TSLA 420.69 call but som...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>eijgnp</td>\n",
       "      <td>28</td>\n",
       "      <td>TESLA delivery numbers thread</td>\n",
       "      <td>So what is everyone thinking for the 4th quarter? Numbers should be comming ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eipbbd</td>\n",
       "      <td>50</td>\n",
       "      <td>New Years Challenge: 5K Make-it or Break-it</td>\n",
       "      <td>Alright WSB,\\nSo I recently came into about 5k due to some nice work stuff. ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>einwh5</td>\n",
       "      <td>37</td>\n",
       "      <td>Literally Free Money</td>\n",
       "      <td>Im new to this stonks shit but stonks literally only go up. Just look at the...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eipkeg</td>\n",
       "      <td>8</td>\n",
       "      <td>Change my mind: Nothing is priced in, Stock market will always go up... Even...</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>einzb0</td>\n",
       "      <td>2</td>\n",
       "      <td>I wannabe make money be rich</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>eig4bp</td>\n",
       "      <td>0</td>\n",
       "      <td>The sad irony of tendies</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>eioqkv</td>\n",
       "      <td>13</td>\n",
       "      <td>Exclusive: Airbus beats goal with 863 jet deliveries in 2019, ousts Boeing f...</td>\n",
       "      <td>[deleted]\\n</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eiop5s</td>\n",
       "      <td>1</td>\n",
       "      <td>Exclusive: Airbus beats goal with 863 jet deliveries in 2019, ousts Boeing f...</td>\n",
       "      <td>[removed]\\n</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   ups  \\\n",
       "100  eifobk     0   \n",
       "64   eijzxq     1   \n",
       "21   eio958     3   \n",
       "61   eik56h     1   \n",
       "30   einvbk    44   \n",
       "31   einsgv     5   \n",
       "60   eikbad     1   \n",
       "67   eijht4     1   \n",
       "34   einjt5     3   \n",
       "36   einfso     4   \n",
       "52   eile0m     2   \n",
       "97   eifu11    15   \n",
       "40   eimo9e     1   \n",
       "35   einh9z     2   \n",
       "25   eio01g     1   \n",
       "58   eikk0r     1   \n",
       "38   ein4nt     1   \n",
       "41   eimigb     0   \n",
       "98   eiftuc     9   \n",
       "75   eiiq38     7   \n",
       "86   eihyh9     0   \n",
       "89   eih880     1   \n",
       "4    eipn7c    11   \n",
       "66   eijlkn    50   \n",
       "69   eij4ma     1   \n",
       "50   eilej0     1   \n",
       "1    eipxm4     0   \n",
       "2    eipw3g     5   \n",
       "51   eileei   407   \n",
       "47   eilpv9     1   \n",
       "27   einz9m   173   \n",
       "84   eii46z     1   \n",
       "15   eiop2z     6   \n",
       "83   eii4ys     1   \n",
       "81   eiicnv   557   \n",
       "94   eig6iz    41   \n",
       "10   eip6af    27   \n",
       "0    eipxnr     6   \n",
       "43   eim7za     1   \n",
       "96   eifxax    13   \n",
       "9    eip6bx     3   \n",
       "16   eioo45     1   \n",
       "44   eim1g2    86   \n",
       "20   eiogf6     1   \n",
       "33   einqmq     1   \n",
       "45   eim0fn     1   \n",
       "73   eiis10    57   \n",
       "85   eihzxb     1   \n",
       "48   eilorf     1   \n",
       "62   eik4ba     1   \n",
       "28   einx0a     5   \n",
       "92   eigfbf     1   \n",
       "23   eio4c2     0   \n",
       "91   eiglvd     1   \n",
       "90   eiguo4     1   \n",
       "18   eiojmk     6   \n",
       "88   eih9st    37   \n",
       "32   einrlu    64   \n",
       "49   eilld6     2   \n",
       "22   eio7dn  1899   \n",
       "37   ein5hs     4   \n",
       "39   ein18r     0   \n",
       "55   eil2hi    25   \n",
       "46   eilqxl     7   \n",
       "87   eihjak  4194   \n",
       "70   eij248    39   \n",
       "68   eijgnp    28   \n",
       "7    eipbbd    50   \n",
       "29   einwh5    37   \n",
       "5    eipkeg     8   \n",
       "26   einzb0     2   \n",
       "95   eig4bp     0   \n",
       "13   eioqkv    13   \n",
       "14   eiop5s     1   \n",
       "\n",
       "                                                                               title  \\\n",
       "100                                                      Hot inside tip AMD crashing   \n",
       "64                                               What do you think about CD Projekt?   \n",
       "21           Is Robinhood a good platform for buying small amounts of stock for kids   \n",
       "61                                               What do you think about CD Projekt?   \n",
       "30                                               Advanced portfolio funding strategy   \n",
       "31                                   Teach me your ways - Shorting Dry Stock Vessels   \n",
       "60                                               What do you think about CD Projekt?   \n",
       "67   A WSB lurker's 2019 results; primarily via position trading beaten down biot...   \n",
       "34                          Index funds donâ€™t run same risk of assignment as stocks?   \n",
       "36                                                    Phase 1 trade deal charity bet   \n",
       "52   Aurora Cannabis (ACB) announces $100 billion partnership deal with Coca-Cola...   \n",
       "97                    DD: AZN / MRK Got Approval For New pancreatic cancer treatment   \n",
       "40                                                   Top Tech Stocks 2020 under $100   \n",
       "35   Priced in? Chinaâ€™s central bank inject $115 billion into the financial syste...   \n",
       "25                                                                 Financial History   \n",
       "58                                                   dO yOu ThiNk Im PlaYing ArOuNd?   \n",
       "38                     Thanks for the hangover heart attack this morning, E*Trade...   \n",
       "41                                                     Please help now please assist   \n",
       "98                                                            Dumbest trades of 2019   \n",
       "75                                      I've concluded we're all autistic dumbasses.   \n",
       "86                                                          Noob getting started....   \n",
       "89   [SERIOUS] So, I've seen this picture on one of the most upvoted posts. Can s...   \n",
       "4                                                                    I will remember   \n",
       "66                                                      Can I buy calls on my alarm?   \n",
       "69                                         Oh how I wish the market was open today ðŸ˜¥   \n",
       "50                                                 My Wife: Spy Calls, Tesla Puts...   \n",
       "1                                i wanna buy a call but i don't want to bet too much   \n",
       "2                                                                      Buy INTU - DD   \n",
       "51                        How we can all be retarded millionaires by the end of 2020   \n",
       "47                                                               ACB gonna moon boys   \n",
       "27   Markets are on the cusp of a correction and there's opportunity for mega ten...   \n",
       "84                            Taking some time today to remember the original autist   \n",
       "15                                                                  DD on pot stocks   \n",
       "83   Whoever was on here talking about capes being popular was onto something! Ch...   \n",
       "81                                                         You people have ruined me   \n",
       "94                                                                   2019 in review.   \n",
       "10                                How many of you tism's are doing the UPRO/TMF mix?   \n",
       "0                                                          Good time to get on $BLUE   \n",
       "43                                                          FCEL Pull and Pray time?   \n",
       "96                                                         Happy new decade and shit   \n",
       "9                                               New strat literally can't go tits up   \n",
       "16             New year, new baseline, new records. We can only go up!, to the moon!   \n",
       "44   Real reason Intel why failed to deliver 10nm for so long is because of feminism   \n",
       "20                                                        Shitpost Long $LULU $BECKY   \n",
       "33                                Playing the long game, soon I'll be a millionaire.   \n",
       "45                                                 How Long Until We See Rivian IPO?   \n",
       "73                                               So whatâ€™s the deal with â€˜analystsâ€™?   \n",
       "85                                                                    ACB, whatâ€™s up   \n",
       "48                                                                       AIMT Timing   \n",
       "62                                                                Government's cutðŸ‡ºðŸ‡¸   \n",
       "28                                                               PSN Parsons Company   \n",
       "92                                                                     MRO Breakout?   \n",
       "23                                                                        STOCK SALE   \n",
       "91                                               Which one of you was this ? (Tesla)   \n",
       "90                                              How many of you are on the spectrum?   \n",
       "18                                                                   AIMT Discussion   \n",
       "88                                        Daily Discussion Thread - January 01, 2020   \n",
       "32                                          What Are Your Moves Tomorrow, January 02   \n",
       "49                                Withhold taxes all year, it's cheaper than margin.   \n",
       "22                                                   Day Trading, how I lost it all.   \n",
       "37                                                     Pattern Day Trader workaround   \n",
       "39                          Best stock to take advantage of undervalued commodities?   \n",
       "55                           Friendly reminder futures markets open at 5PM CT/6PM ET   \n",
       "46                                                                       AIMT Timing   \n",
       "87                                     2019 1st Annual Wallstreetbets Awards Winners   \n",
       "70                           How to trade like a professional stonks autist in 2020.   \n",
       "68                                                     TESLA delivery numbers thread   \n",
       "7                                        New Years Challenge: 5K Make-it or Break-it   \n",
       "29                                                              Literally Free Money   \n",
       "5    Change my mind: Nothing is priced in, Stock market will always go up... Even...   \n",
       "26                                                      I wannabe make money be rich   \n",
       "95                                                          The sad irony of tendies   \n",
       "13   Exclusive: Airbus beats goal with 863 jet deliveries in 2019, ousts Boeing f...   \n",
       "14   Exclusive: Airbus beats goal with 863 jet deliveries in 2019, ousts Boeing f...   \n",
       "\n",
       "                                                                            selftext  \\\n",
       "100                                                                      [removed]\\n   \n",
       "64                                                                       [removed]\\n   \n",
       "21                                                                       [deleted]\\n   \n",
       "61                                                                       [removed]\\n   \n",
       "30                                                                       [deleted]\\n   \n",
       "31   I'm pretty new to investing, so obviously my only experience is 2-3 weeks on...   \n",
       "60                                                                       [removed]\\n   \n",
       "67                                                                       [deleted]\\n   \n",
       "34   Warning: actual useful information \\nStudying up on iron butterflies and it ...   \n",
       "36   Tards:\\nA few days ago, in response to news that the Phase1 trade deal is sc...   \n",
       "52                                                                       [removed]\\n   \n",
       "97                                                                       [deleted]\\n   \n",
       "40                                                                       [removed]\\n   \n",
       "35                                                                       [deleted]\\n   \n",
       "25                                                                       [deleted]\\n   \n",
       "58                                                                       [deleted]\\n   \n",
       "38                                                                       [deleted]\\n   \n",
       "41                                                                       [removed]\\n   \n",
       "98                                                                       [deleted]\\n   \n",
       "75                                                                       [removed]\\n   \n",
       "86                                                                       [removed]\\n   \n",
       "89                                                                       [deleted]\\n   \n",
       "4                                                                        [removed]\\n   \n",
       "66   So I left my alarm on this morning and I kept hitting snooze as I usually do...   \n",
       "69                                                                       [removed]\\n   \n",
       "50                                                                       [removed]\\n   \n",
       "1                                                                        [removed]\\n   \n",
       "2    https://www.schaeffersresearch.com/content/analysis/2019/12/31/intu-options-...   \n",
       "51   Literally only buy calls\\nFucking ACB is trending down? Fuck it calls.\\nFuck...   \n",
       "47                                                                       [removed]\\n   \n",
       "27   I was going to tag this as technical, but since this sub misunderstands hate...   \n",
       "84                                                                       [deleted]\\n   \n",
       "15   First day of legal weed in Illinois, passed by a store on lunchtime and saw ...   \n",
       "83                                                                       [removed]\\n   \n",
       "81   Every time I talk in public or write some message I start talking like a god...   \n",
       "94   2019 was a wild ride for this degenerate sub. I learned some lessons which t...   \n",
       "10   I saw an older thread on it, curious who is still doing it? How have your re...   \n",
       "0                                                                        [deleted]\\n   \n",
       "43                                                                       [removed]\\n   \n",
       "96                                                                       [removed]\\n   \n",
       "9                                                                        [deleted]\\n   \n",
       "16                                                                       [removed]\\n   \n",
       "44                                                                       [removed]\\n   \n",
       "20                                                                       [deleted]\\n   \n",
       "33                                                                       [deleted]\\n   \n",
       "45                                                                       [removed]\\n   \n",
       "73                                                                       [deleted]\\n   \n",
       "85                                                                       [removed]\\n   \n",
       "48                                                                       [removed]\\n   \n",
       "62                                                                       [removed]\\n   \n",
       "28                                                                       [deleted]\\n   \n",
       "92                                                                       [removed]\\n   \n",
       "23                                                                       [removed]\\n   \n",
       "91                                                                       [removed]\\n   \n",
       "90                                                                       [removed]\\n   \n",
       "18                                                                       [deleted]\\n   \n",
       "88   Your daily trading discussion thread. Please keep the shitposting to a minim...   \n",
       "32   Daily Discussion Thread - DD - YOLO - Discussion\\nWeekly Earnings Discussion...   \n",
       "49                                                                       [removed]\\n   \n",
       "22                                                                       [removed]\\n   \n",
       "37   If you trade options in a non margin account, PDT wont apply. Option trades ...   \n",
       "39   General consensus is that commodities are way undervalued right now. I've be...   \n",
       "55   Hope you recovered enough from your hangover. Let's get ready for another bu...   \n",
       "46   So we all know that AR101 is scheduled to be approved \"late January\" and we ...   \n",
       "87   Welcome to the 1st Annual Wallstreetbets Awards!\\nHere we will celebrate som...   \n",
       "70   We all love dropping all our $BECKY bucks on a good TSLA 420.69 call but som...   \n",
       "68   So what is everyone thinking for the 4th quarter? Numbers should be comming ...   \n",
       "7    Alright WSB,\\nSo I recently came into about 5k due to some nice work stuff. ...   \n",
       "29   Im new to this stonks shit but stonks literally only go up. Just look at the...   \n",
       "5                                                                        [deleted]\\n   \n",
       "26                                                                       [removed]\\n   \n",
       "95                                                                       [removed]\\n   \n",
       "13                                                                       [deleted]\\n   \n",
       "14                                                                       [removed]\\n   \n",
       "\n",
       "     clusters  \n",
       "100         0  \n",
       "64          0  \n",
       "21          0  \n",
       "61          0  \n",
       "30          0  \n",
       "31          0  \n",
       "60          0  \n",
       "67          0  \n",
       "34          0  \n",
       "36          0  \n",
       "52          0  \n",
       "97          0  \n",
       "40          0  \n",
       "35          0  \n",
       "25          0  \n",
       "58          1  \n",
       "38          1  \n",
       "41          1  \n",
       "98          1  \n",
       "75          1  \n",
       "86          1  \n",
       "89          1  \n",
       "4           1  \n",
       "66          2  \n",
       "69          2  \n",
       "50          2  \n",
       "1           2  \n",
       "2           2  \n",
       "51          2  \n",
       "47          2  \n",
       "27          3  \n",
       "84          3  \n",
       "15          3  \n",
       "83          3  \n",
       "81          3  \n",
       "94          3  \n",
       "10          3  \n",
       "0           4  \n",
       "43          4  \n",
       "96          5  \n",
       "9           5  \n",
       "16          5  \n",
       "44          6  \n",
       "20          6  \n",
       "33          6  \n",
       "45          6  \n",
       "73          7  \n",
       "85          7  \n",
       "48          8  \n",
       "62          8  \n",
       "28          8  \n",
       "92          8  \n",
       "23          8  \n",
       "91          8  \n",
       "90          8  \n",
       "18          8  \n",
       "88          9  \n",
       "32          9  \n",
       "49         10  \n",
       "22         10  \n",
       "37         10  \n",
       "39         11  \n",
       "55         11  \n",
       "46         11  \n",
       "87         11  \n",
       "70         11  \n",
       "68         11  \n",
       "7          11  \n",
       "29         11  \n",
       "5          11  \n",
       "26         12  \n",
       "95         12  \n",
       "13         13  \n",
       "14         13  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pd.options.display.max_colwidth = 80\n",
    "pd.options.display.max_rows = num_rows\n",
    "#post_clouded['selftext_trunc'] = post_clouded.selftext.apply( lambda x : x[:cut_off])\n",
    "#post_clouded[['id', 'ups', 'title', 'selftext_trunc', 'clusters']].sort_values(by = 'clusters')\n",
    "post_clouded[['id', 'ups', 'title', 'selftext',  'clusters']].sort_values(by = 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ad4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfffbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a696a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ecb594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "251d2c46",
   "metadata": {},
   "source": [
    "### There are too many posts to cluster the entire dataframe. Instead, we can first group posts by time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "520679a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,300) (4,300) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-12c2330e7dbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpost_clouded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcloudify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_off\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcut_off\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcloud_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'selftext_point_cloud'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'title_point_cloud'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merged_point_cloud'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-87a1904d4794>\u001b[0m in \u001b[0;36mcloudify\u001b[1;34m(df, model_dict, cut_off)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_point_cloud'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0munpack_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_off\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'selftext_point_cloud'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_selftext'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0munpack_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_off\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'merged_point_cloud'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_point_cloud'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'selftext_point_cloud'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__add__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__radd__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   4996\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4997\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4998\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5000\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;31m# error: \"None\" not callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,300) (4,300) "
     ]
    }
   ],
   "source": [
    "num_rows = 100\n",
    "cut_off = 30\n",
    "df_slice = pd.DataFrame(df.loc[:num_rows, :])\n",
    "\n",
    "post_clouded = cloudify(df_slice, model_dict, cut_off = cut_off)\n",
    "cloud_col = ['selftext_point_cloud','title_point_cloud', 'merged_point_cloud'][2]\n",
    "\n",
    "post_clouded = pd.DataFrame(post_clouded [ post_clouded[cloud_col].apply(lambda x : len(x) > 0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69d4f3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'title', 'author', 'created_datetime_utc', 'selftext',\n",
       "       'url', 'upvote_ratio', 'ups', 'total_awards_received', 'num_comments',\n",
       "       'num_crossposts', 'is_self', 'is_video', 'media_only', 'id',\n",
       "       'created_utc', 'selftext_html', 'author_fullname', 'tokenized_title',\n",
       "       'tokenized_selftext', 'title_point_cloud', 'selftext_point_cloud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_clouded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "832e618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_clouded= post_clouded.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b4ff35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_graph = nx.Graph()\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i].id in post_clouded.index:\n",
    "        post_graph.add_node(df.iloc[i].id)\n",
    "        for j in range(-200,200): # should replace by posting time?\n",
    "            if j >= 0 and j < len(df) and i != j:\n",
    "                if df.iloc[j].id in post_clouded.index:\n",
    "                    post_graph.add_edge( df.iloc[i].id, df.iloc[j].id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "081f1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-9e6aa5f724e3>:130: RuntimeWarning: invalid value encountered in true_divide\n",
      "  M /= M.max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repost: ('ej4arj', 'ej1h8d')\n",
      "Repost: ('ej4arj', 'ejdo7p')\n",
      "Repost: ('ej4arj', 'ejdnps')\n"
     ]
    }
   ],
   "source": [
    "possible_reposts = []\n",
    "\n",
    "for edge in post_graph.edges():\n",
    "    distance = ot_distance(post_clouded.loc[edge[0]][cloud_col], post_clouded.loc[edge[1]][cloud_col])\n",
    "    post_graph.edges[edge][\"distance\"] = distance\n",
    "    if distance == 0:\n",
    "        print(\"Repost:\", edge)\n",
    "        post_graph.edges[edge][\"affinity\"] = 99999999999999999\n",
    "    else:\n",
    "        post_graph.edges[edge][\"affinity\"] = 1/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab7794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_list = list(post_graph.nodes())\n",
    "aff_matrix = nx.adj_matrix(post_graph, nodelist = node_list)\n",
    "sc = SpectralClustering(n_clusters = int(len(post_clouded)/5), affinity = 'precomputed')\n",
    "labels = sc.fit_predict(aff_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87176f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_df = pd.DataFrame()\n",
    "post_df['id'] = node_list\n",
    "post_df['title'] = [post_clouded.loc[post_id].title for post_id in node_list]\n",
    "post_df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865db8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.sort_values(by = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0\n",
    "for e in post_graph.edges():\n",
    "    if post_graph.edges[e][\"distance\"] <= epsilon:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "62707ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eibdob', 'ejpe44')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "34a8e8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158603238983451"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " post_graph.edges[e][\"distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a64eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
