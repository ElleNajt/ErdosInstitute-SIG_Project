{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\n",
    "\n",
    "    https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/#complete-tutorial-on-how-to-use-gpt2-for-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b82a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/ai-innovation/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ideas:\n",
    "    \n",
    "    1. Use huggingface to predict duds, use this as a feature\n",
    "    2.\n",
    "    \n",
    "Resources:\n",
    "    \n",
    "    https://jalammar.github.io/illustrated-gpt2/\n",
    "        https://jalammar.github.io/illustrated-transformer/\n",
    "            \n",
    "            \n",
    "            https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e054f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf3cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77691ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
