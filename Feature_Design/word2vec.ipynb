{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@black_swan/how-to-train-word2vec-and-fasttext-embedding-on-wikipedia-corpus-9e8ac45a0c0a\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860c150",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac31417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7640b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/wsb_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5710dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-zA-Z ]')\n",
    "for col in ['title', 'author', 'selftext']:\n",
    "    df[col] = df[col].apply(lambda x : regex.sub('', str(x) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e17736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   Good time to get on BLUE\n",
       "1          i wanna buy a call but i dont want to bet too ...\n",
       "2                                               Buy INTU  DD\n",
       "3          If you thought your  was bad at least you aren...\n",
       "4                                            I will remember\n",
       "                                 ...                        \n",
       "1260232                        ASO technical breakout and DD\n",
       "1260233                                   First YOLO on CRSR\n",
       "1260234                                      Rkt to the moon\n",
       "1260235    My therapist told me Im delusional for thinkin...\n",
       "1260236    CCIV  Lucid motors testing their FSD in Fremon...\n",
       "Name: title, Length: 1260237, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28ad807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6025efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "\n",
    "text = df['title']\n",
    "#sentences = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    "sentences = [x.split(' ') for x in text]\n",
    "model = gensim.models.Word2Vec(sentences = sentences,  min_count=10, vector_size=20, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d8883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/29759 is \n",
      "word #1/29759 is the\n",
      "word #2/29759 is to\n",
      "word #3/29759 is I\n",
      "word #4/29759 is a\n",
      "word #5/29759 is and\n",
      "word #6/29759 is GME\n",
      "word #7/29759 is is\n",
      "word #8/29759 is on\n",
      "word #9/29759 is of\n",
      "word #10/29759 is in\n",
      "word #11/29759 is for\n",
      "word #12/29759 is this\n",
      "word #13/29759 is you\n",
      "word #14/29759 is my\n",
      "word #15/29759 is it\n",
      "word #16/29759 is AMC\n",
      "word #17/29759 is are\n",
      "word #18/29759 is with\n",
      "word #19/29759 is THE\n",
      "word #20/29759 is buy\n",
      "word #21/29759 is all\n",
      "word #22/29759 is at\n",
      "word #23/29759 is we\n",
      "word #24/29759 is up\n",
      "word #25/29759 is Im\n",
      "word #26/29759 is The\n",
      "word #27/29759 is me\n",
      "word #28/29759 is be\n",
      "word #29/29759 is but\n",
      "word #30/29759 is that\n",
      "word #31/29759 is Robinhood\n",
      "word #32/29759 is not\n",
      "word #33/29759 is stock\n",
      "word #34/29759 is from\n",
      "word #35/29759 is have\n",
      "word #36/29759 is do\n",
      "word #37/29759 is TO\n",
      "word #38/29759 is like\n",
      "word #39/29759 is just\n",
      "word #40/29759 is now\n",
      "word #41/29759 is will\n",
      "word #42/29759 is about\n",
      "word #43/29759 is can\n",
      "word #44/29759 is What\n",
      "word #45/29759 is HOLD\n",
      "word #46/29759 is moon\n",
      "word #47/29759 is what\n",
      "word #48/29759 is or\n",
      "word #49/29759 is out\n",
      "word #50/29759 is get\n",
      "word #51/29759 is your\n",
      "word #52/29759 is going\n",
      "word #53/29759 is go\n",
      "word #54/29759 is today\n",
      "word #55/29759 is This\n",
      "word #56/29759 is WSB\n",
      "word #57/29759 is more\n",
      "word #58/29759 is shares\n",
      "word #59/29759 is its\n",
      "word #60/29759 is NOK\n",
      "word #61/29759 is BB\n",
      "word #62/29759 is they\n",
      "word #63/29759 is market\n",
      "word #64/29759 is k\n",
      "word #65/29759 is A\n",
      "word #66/29759 is here\n",
      "word #67/29759 is money\n",
      "word #68/29759 is stocks\n",
      "word #69/29759 is How\n",
      "word #70/29759 is so\n",
      "word #71/29759 is as\n",
      "word #72/29759 is BUY\n",
      "word #73/29759 is us\n",
      "word #74/29759 is if\n",
      "word #75/29759 is next\n",
      "word #76/29759 is was\n",
      "word #77/29759 is We\n",
      "word #78/29759 is time\n",
      "word #79/29759 is Is\n",
      "word #80/29759 is some\n",
      "word #81/29759 is one\n",
      "word #82/29759 is has\n",
      "word #83/29759 is down\n",
      "word #84/29759 is short\n",
      "word #85/29759 is by\n",
      "word #86/29759 is still\n",
      "word #87/29759 is guys\n",
      "word #88/29759 is sell\n",
      "word #89/29759 is new\n",
      "word #90/29759 is know\n",
      "word #91/29759 is dont\n",
      "word #92/29759 is trading\n",
      "word #93/29759 is buying\n",
      "word #94/29759 is hold\n",
      "word #95/29759 is an\n",
      "word #96/29759 is MOON\n",
      "word #97/29759 is into\n",
      "word #98/29759 is how\n",
      "word #99/29759 is when\n",
      "word #100/29759 is right\n",
      "word #101/29759 is holding\n",
      "word #102/29759 is Lets\n",
      "word #103/29759 is make\n",
      "word #104/29759 is Just\n",
      "word #105/29759 is My\n",
      "word #106/29759 is think\n",
      "word #107/29759 is bought\n",
      "word #108/29759 is only\n",
      "word #109/29759 is Why\n",
      "word #110/29759 is back\n",
      "word #111/29759 is AND\n",
      "word #112/29759 is Buy\n",
      "word #113/29759 is their\n",
      "word #114/29759 is You\n",
      "word #115/29759 is should\n",
      "word #116/29759 is after\n",
      "word #117/29759 is week\n",
      "word #118/29759 is If\n",
      "word #119/29759 is IS\n",
      "word #120/29759 is good\n",
      "word #121/29759 is Its\n",
      "word #122/29759 is got\n",
      "word #123/29759 is WE\n",
      "word #124/29759 is calls\n",
      "word #125/29759 is day\n",
      "word #126/29759 is am\n",
      "word #127/29759 is our\n",
      "word #128/29759 is Hold\n",
      "word #129/29759 is no\n",
      "word #130/29759 is To\n",
      "word #131/29759 is much\n",
      "word #132/29759 is over\n",
      "word #133/29759 is RH\n",
      "word #134/29759 is i\n",
      "word #135/29759 is need\n",
      "word #136/29759 is retards\n",
      "word #137/29759 is tomorrow\n",
      "word #138/29759 is people\n",
      "word #139/29759 is them\n",
      "word #140/29759 is Can\n",
      "word #141/29759 is help\n",
      "word #142/29759 is DD\n",
      "word #143/29759 is who\n",
      "word #144/29759 is been\n",
      "word #145/29759 is these\n",
      "word #146/29759 is want\n",
      "word #147/29759 is see\n",
      "word #148/29759 is options\n",
      "word #149/29759 is squeeze\n",
      "word #150/29759 is too\n",
      "word #151/29759 is YOU\n",
      "word #152/29759 is price\n",
      "word #153/29759 is why\n",
      "word #154/29759 is Dont\n",
      "word #155/29759 is way\n",
      "word #156/29759 is there\n",
      "word #157/29759 is doing\n",
      "word #158/29759 is GameStop\n",
      "word #159/29759 is off\n",
      "word #160/29759 is New\n",
      "word #161/29759 is PLTR\n",
      "word #162/29759 is dip\n",
      "word #163/29759 is ON\n",
      "word #164/29759 is before\n",
      "word #165/29759 is THIS\n",
      "word #166/29759 is again\n",
      "word #167/29759 is made\n",
      "word #168/29759 is cant\n",
      "word #169/29759 is YOLO\n",
      "word #170/29759 is share\n",
      "word #171/29759 is TSLA\n",
      "word #172/29759 is NOT\n",
      "word #173/29759 is fucking\n",
      "word #174/29759 is first\n",
      "word #175/29759 is than\n",
      "word #176/29759 is last\n",
      "word #177/29759 is They\n",
      "word #178/29759 is take\n",
      "word #179/29759 is When\n",
      "word #180/29759 is did\n",
      "word #181/29759 is So\n",
      "word #182/29759 is puts\n",
      "word #183/29759 is Short\n",
      "word #184/29759 is selling\n",
      "word #185/29759 is would\n",
      "word #186/29759 is Whats\n",
      "word #187/29759 is big\n",
      "word #188/29759 is account\n",
      "word #189/29759 is US\n",
      "word #190/29759 is boys\n",
      "word #191/29759 is any\n",
      "word #192/29759 is It\n",
      "word #193/29759 is SNDL\n",
      "word #194/29759 is hands\n",
      "word #195/29759 is IN\n",
      "word #196/29759 is Not\n",
      "word #197/29759 is anyone\n",
      "word #198/29759 is Stock\n",
      "word #199/29759 is tendies\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(model.wv.index_to_key):\n",
    "    if index == 200:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3555599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa25e0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebc52aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"AMC\"\t- similarity: 0.915942\n",
      "\n",
      "\"shares\"\t- similarity: 0.835391\n",
      "\n",
      "\"gme\"\t- similarity: 0.802144\n",
      "\n",
      "\"amc\"\t- similarity: 0.801782\n",
      "\n",
      "\"dip\"\t- similarity: 0.778836\n",
      "\n",
      "\"everything\"\t- similarity: 0.759182\n",
      "\n",
      "\"puts\"\t- similarity: 0.756371\n",
      "\n",
      "\"calls\"\t- similarity: 0.753323\n",
      "\n",
      "\"today\"\t- similarity: 0.750671\n",
      "\n",
      "\"yesterday\"\t- similarity: 0.749364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word, sim in model.wv.most_similar(positive=['GME'], negative=[]):\n",
    "    print('\\\"%s\\\"\\t- similarity: %g' % (word, sim))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b393a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"gains\"\t- similarity: 0.876917\n",
      "\n",
      "\"profits\"\t- similarity: 0.868857\n",
      "\n",
      "\"losses\"\t- similarity: 0.822006\n",
      "\n",
      "\"hands\"\t- similarity: 0.819225\n",
      "\n",
      "\"money\"\t- similarity: 0.808949\n",
      "\n",
      "\"stonks\"\t- similarity: 0.793267\n",
      "\n",
      "\"ass\"\t- similarity: 0.792297\n",
      "\n",
      "\"cause\"\t- similarity: 0.789839\n",
      "\n",
      "\"friends\"\t- similarity: 0.785216\n",
      "\n",
      "\"rich\"\t- similarity: 0.783376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word, sim in model.wv.most_similar(positive=['tendies']):\n",
    "    print('\\\"%s\\\"\\t- similarity: %g' % (word, sim))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cfc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and loading\n",
    "\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260d4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c2928c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9500aa508bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mx_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_dimensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_with_plotly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_in_notebook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-9500aa508bd9>\u001b[0m in \u001b[0;36mreduce_dimensions\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# reduce using t-SNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mx_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Erdos_Institute\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \"\"\"\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Erdos_Institute\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mdegrees_of_freedom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         return self._tsne(P, degrees_of_freedom, n_samples,\n\u001b[0m\u001b[0;32m    842\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Erdos_Institute\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_iter_without_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0m\u001b[0;32m    898\u001b[0m                                                           **opt_args)\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Erdos_Institute\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compute_error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Erdos_Institute\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[0m\u001b[0;32m    264\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055eff43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
