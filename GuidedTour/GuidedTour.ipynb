{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1757857c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "os.chdir(\"../Automated/\")\n",
    "from DataGathering import RedditScraper\n",
    "from ChangePointAnalysis import ChangePointAnalysis\n",
    "from NeuralNets import CreateNeuralNets\n",
    "\n",
    "\n",
    "subreddit = \"WallStreetBets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da2fbb",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "For analyzing wallstreetbets data, we recommend downloading full.csv from [url] and putting it in ../Data/subreddit_wallstreetbets.\n",
    "\n",
    "If you want to scrape a different subreddit, you can use the following file. You will need API.env with appropriate credentials in /Automated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = dt.datetime(2020, 1, 1)\n",
    "end =  dt.datetime(2020, 1, 30)\n",
    "\n",
    "if not os.path.exists(f\"../Data/subreddit_{subreddit}/full.csv\"):\n",
    "    print(\"Did not find scraped data, scraping.\")\n",
    "\n",
    "    RedditScraper.scrape_data(subreddits = [subreddit], start = start, end = end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77e206",
   "metadata": {},
   "source": [
    "# Change Point Analysis\n",
    "\n",
    "The next cell will open full.csv , compute the words that are among the top daily_words most popular words on any day, and then run the change point analysis model on each of them.\n",
    "\n",
    "\n",
    "The first time this is a run, a cleaned up version of the dataframe will be created for ease of processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4a6ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the changepoints:\n",
      "working on  WallStreetBets\n",
      "Did not find preprocessed version, preprocessing. (This will only be done once.)\n",
      "['debate', 'nio', 'virus', 'wsb', 'moon', 'biden', 'gme', 'sos', 'gang', 'merry', 'nkla', 'mt', 'two', 'week', 'tesla', 'tsla', 'go', 'currency', 'year', 'detroit', 'war', 'robinhood', 'calls', 'mascot', 'amc', 'azn', 'rh', 'citron', 'trump', 'mnmd', 'spy', 'wkhs', 'test', 'stock', 'pltr', 'spce', 'nikola', 'bezos', 'dd', 'like', 'mvis', 'ev', 'next', 'fortnite', 'kodk', 'bb', 'options', 'month', 'bear', 'election', 'new', 'slv', 'questrade', 'coin', 'stocks', 'christmas', 'short', 'covid', 'twitter', 'gold', 'dogecoin', 'hold', 'july', 'earnings', 'msft', 'today', 'get', 'one', 'fed', 'day', 'prpl', 'may', 'iran', 'sndl', 'trading', 'closed', 'rkt', 'tendies', 'tomorrow', 'trevor', 'buy', 'puts', 'im', 'suicide', 'snap', 'kodak', 'k', 'market', 'clov', 'plug', 'money', 'amd', 'monday', 'log', 'coronavirus', 'oil', 'apes', 'elon', 'uso', 'jpow'] pop_words\n",
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\n",
      "working on debate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\\ChangePointAnalysis\\BayesianMethods.py:134: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(steps, tune=tune, step = step)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [delta]\n",
      ">Metropolis: [change_point_beta]\n",
      ">Metropolis: [tau_beta]\n",
      ">Metropolis: [beta_2]\n",
      ">Metropolis: [alpha_2]\n",
      ">Metropolis: [beta_1]\n",
      ">Metropolis: [alpha_1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='26212' class='' max='140000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      18.72% [26212/140000 00:54<03:58 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "up_to = 2 # Only calculate change points for up_to of the popular words. Set to None to do all of them.\n",
    "daily_words = 2 # Get the daily_words most popular posts on each day.\n",
    "\n",
    "print('Computing the changepoints:')\n",
    "ChangePointAnalysis.changepointanalysis([subreddit], up_to = up_to, daily_words = daily_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd526f",
   "metadata": {},
   "source": [
    "After running, these files will in ../Data/subreddit_subreddit/Changepoints/Metropolis_30000Draws_5000Tune\n",
    "\n",
    "(The final folder corresponds to the parameters of the Markov chain used by pymc3 for the inference.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfacb1",
   "metadata": {},
   "source": [
    "For instance: \n",
    "    \n",
    "![title](../Data/subreddit_WallStreetBets/Changepoints/Metropolis_30000Draws_5000Tune/ChangePoint_tendies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01619fa7",
   "metadata": {},
   "source": [
    "### Brief explanation of how this works:\n",
    "\n",
    "The Bayesian model is as follows:\n",
    "\n",
    "1. A coin is flipped with probability p.\n",
    "2. If the coin comes up heads, then there is a change point. Otherwise, there is no change point.\n",
    "3. It is assumed that the frequency random variable consists of independent draws from a beta distribution. If the coin decided there would be no change point, it is the same beta distribution at all times. Otherwise, it is a different beta on the different sides of the change points.\n",
    "\n",
    "The posterior distribution of p is the models confidence that there is a change point, and the posterior distribution of tau represents its guess about when it occured.\n",
    "\n",
    "Of course, this is not a realistic picture of the process; the independence of the different draws from the betas is especially unlike the data. However, it appears to be good enough to discover change points.\n",
    "\n",
    "As currently written, it only handles one change point, however this can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdfd9cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, ','.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-08e4922cc3e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataframe_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"../Data/subreddit_{subreddit}/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpreprocessed_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"changepoint_preprocessed.pkl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[1;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;31m# e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, ','."
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_FOLDER = f\"../Data/subreddit_{subreddit}/Changepoints/metropolis_30000Draws_5000Tune/\"\n",
    "dataframe_path = f\"../Data/subreddit_{subreddit}/\"\n",
    "preprocessed_location = dataframe_path + \"changepoint_preprocessed.pkl\"\n",
    "pd.read_pickle(preprocessed_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d459be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d71d5072",
   "metadata": {},
   "source": [
    "# Neural Nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87793f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, accuracies, word_tokenizer, df = CreateNeuralNets.buildnets([subreddit])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7efb3",
   "metadata": {},
   "source": [
    "## Predicted popularity as a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3934b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"GME GME GME GME GME GME\"\n",
    "CreateNeuralNets.timeseries(df, text, model, word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508bc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
