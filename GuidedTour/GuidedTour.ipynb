{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80601a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "os.chdir(\"../Automated/\")\n",
    "from DataGathering import RedditScraper\n",
    "from ChangePointAnalysis import ChangePointAnalysis\n",
    "from NeuralNets import CreateNeuralNets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "subreddit = \"WallStreetBets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d87b64",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "For analyzing wallstreetbets data, we recommend downloading full.csv from [url] and putting it in ../Data/subreddit_wallstreetbets.\n",
    "\n",
    "If you want to scrape a different subreddit, you can use the following file. You will need API.env with appropriate credentials in /Automated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc661bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = dt.datetime(2020, 1, 1)\n",
    "end =  dt.datetime(2020, 1, 30)\n",
    "\n",
    "if not os.path.exists(f\"../Data/subreddit_{subreddit}/full.csv\"):\n",
    "    print(\"Did not find scraped data, scraping.\")\n",
    "\n",
    "    RedditScraper.scrape_data(subreddits = [subreddit], start = start, end = end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87478f4",
   "metadata": {},
   "source": [
    "# Change Point Analysis\n",
    "\n",
    "The next cell will open full.csv , compute the words that are among the top daily_words most popular words on any day, and then run the change point analysis model on each of them.\n",
    "\n",
    "\n",
    "The first time this is a run, a cleaned up version of the dataframe will be created for ease of processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e579ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the changepoints:\n",
      "working on  WallStreetBets\n",
      "['log', 'kodak', 'short', 'mnmd', 'monday', 'snap', 'month', 'wsb', 'next', 'slv', 'questrade', 'rh', 'gold', 'iran', 'mascot', 'ev', 'covid', 'rkt', 'dd', 'fed', 'biden', 'may', 'apes', 'nikola', 'robinhood', 'nkla', 'elon', 'options', 'buy', 'coronavirus', 'clov', 'msft', 'azn', 'year', 'july', 'suicide', 'citron', 'trevor', 'merry', 'gme', 'detroit', 'plug', 'debate', 'mt', 'tomorrow', 'wkhs', 'test', 'bezos', 'new', 'trading', 'spy', 'two', 'one', 'twitter', 'election', 'get', 'christmas', 'tendies', 'sndl', 'tsla', 'puts', 'oil', 'sos', 'virus', 'war', 'moon', 'spce', 'k', 'week', 'today', 'hold', 'dogecoin', 'uso', 'amd', 'trump', 'earnings', 'mvis', 'jpow', 'im', 'amc', 'bear', 'currency', 'day', 'market', 'pltr', 'tesla', 'kodk', 'stock', 'gang', 'go', 'stocks', 'bb', 'money', 'closed', 'nio', 'coin', 'prpl', 'calls', 'like', 'fortnite'] pop_words\n",
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\n",
      "working on log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\\ChangePointAnalysis\\BayesianMethods.py:134: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(steps, tune=tune, step = step)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [delta]\n",
      ">Metropolis: [change_point_beta]\n",
      ">Metropolis: [tau_beta]\n",
      ">Metropolis: [beta_2]\n",
      ">Metropolis: [alpha_2]\n",
      ">Metropolis: [beta_1]\n",
      ">Metropolis: [alpha_1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='140000' class='' max='140000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [140000/140000 05:55<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 5_000 tune and 30_000 draw iterations (20_000 + 120_000 draws total) took 395 seconds.\n",
      "The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change Point Guess 0.25\n",
      "{'change_point_confidence': [0.25], 'mus': [(0.009015622219098773, 0.011524003066574464)], 'mu_diff': [0.0025083808474756913], 'tau_map': ['2020-01-01'], 'tau_std': [137.09623688628048], 'entropy': [5.218872264011381], 'change_point_guess': [0.25]}\n",
      "appending\n",
      "working on kodak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\\ChangePointAnalysis\\BayesianMethods.py:134: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(steps, tune=tune, step = step)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [delta]\n",
      ">Metropolis: [change_point_beta]\n",
      ">Metropolis: [tau_beta]\n",
      ">Metropolis: [beta_2]\n",
      ">Metropolis: [alpha_2]\n",
      ">Metropolis: [beta_1]\n",
      ">Metropolis: [alpha_1]\n"
     ]
    }
   ],
   "source": [
    "up_to = 1 # Only calculate change points for up_to of the popular words. Set to None to do all of them.\n",
    "daily_words = 2 # Get the daily_words most popular posts on each day.\n",
    "\n",
    "\n",
    "# Compute the changepoints\n",
    "ChangePointAnalysis.changepointanalysis([subreddit], up_to = up_to, daily_words = daily_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f31c07",
   "metadata": {},
   "source": [
    "After running, these files will in ../Data/subreddit_subreddit/Changepoints/Metropolis_30000Draws_5000Tune\n",
    "\n",
    "(The final folder corresponds to the parameters of the Markov chain used by pymc3 for the inference.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceea485",
   "metadata": {},
   "source": [
    "For instance: \n",
    "    \n",
    "![title](../Data/subreddit_WallStreetBets/Changepoints/Metropolis_30000Draws_5000Tune/ChangePoint_virus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aae378",
   "metadata": {},
   "source": [
    "### Brief explanation of how this works:\n",
    "\n",
    "The Bayesian model is as follows:\n",
    "\n",
    "1. A coin is flipped with probability p.\n",
    "2. If the coin comes up heads, then there is a change point. Otherwise, there is no change point.\n",
    "3. It is assumed that the frequency random variable consists of independent draws from a beta distribution. If the coin decided there would be no change point, it is the same beta distribution at all times. Otherwise, it is a different beta on the different sides of the change points.\n",
    "\n",
    "The posterior distribution of p is the models confidence that there is a change point, and the posterior distribution of tau represents its guess about when it occured.\n",
    "\n",
    "Of course, this is not a realistic picture of the process; the independence of the different draws from the betas is especially unlike the data. However, it appears to be good enough to discover change points.\n",
    "\n",
    "As currently written, it only handles one change point, however this can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ec0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92333afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee28b8c",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "\n",
    "The following code will train a neural net that predicts, given a submission's title text and time of posting, whether that submission's score will be above the median. \n",
    "\n",
    "We use pre-trained GloVe word embeddings in order to convert the title text into a vector that can be used in the neural net. These word embeddings are tuned along with the model parameters as the model is being trained. \n",
    "\n",
    "This technique and the neural net's architecture are taken from a blog post of Max Woolf, https://minimaxir.com/2017/06/reddit-deep-learning/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951f5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Post Classification Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (26,32,45,79,82,87,116,117,118,121,122,123,124) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1134/1134 [==============================] - 98s 58ms/step - loss: 0.7454 - main_out_loss: 0.6063 - aux_out_loss: 0.6957 - main_out_accuracy: 0.6723 - aux_out_accuracy: 0.5042 - val_loss: 0.6840 - val_main_out_loss: 0.5480 - val_aux_out_loss: 0.6802 - val_main_out_accuracy: 0.7165 - val_aux_out_accuracy: 0.5756\n",
      "Epoch 2/20\n",
      "1134/1134 [==============================] - 66s 58ms/step - loss: 0.6384 - main_out_loss: 0.5037 - aux_out_loss: 0.6739 - main_out_accuracy: 0.7492 - aux_out_accuracy: 0.5958 - val_loss: 0.6675 - val_main_out_loss: 0.5346 - val_aux_out_loss: 0.6647 - val_main_out_accuracy: 0.7248 - val_aux_out_accuracy: 0.6091\n",
      "Epoch 3/20\n",
      "1134/1134 [==============================] - 81s 72ms/step - loss: 0.5735 - main_out_loss: 0.4437 - aux_out_loss: 0.6494 - main_out_accuracy: 0.7905 - aux_out_accuracy: 0.6490 - val_loss: 0.6856 - val_main_out_loss: 0.5551 - val_aux_out_loss: 0.6523 - val_main_out_accuracy: 0.7222 - val_aux_out_accuracy: 0.6274- loss: 0.5634 - main_ou - ETA: 54s - loss: 0.5642 - main_out_loss: 0.4337 - aux_out_loss: 0.6525 - main_out_accuracy: 0.8024 -  - ETA: 35s - loss: 0.56 - ETA: 19s - loss: 0.5715 - main_out_ - ETA: 5s - loss: 0.5729 - main_out_loss: 0.4429 - aux_out_loss: 0.6497 - main_out_accuracy: 0.7912 - au - ETA: 4s - loss: 0.5730 - main_out_loss: 0.4431 - aux_out_loss: 0.6496 - main_out_accuracy: 0.791 - ETA: 2s - loss: 0.5733 - main_out_loss: 0.4434 - aux_out_loss: 0.6495 - main_out_accuracy: 0.790 - ETA: 0s - loss: 0.5735 - main_out_loss: 0.4436 - aux_out_loss: 0.6494 - main_out_accuracy: 0.7906 - aux_out_accurac\n",
      "Epoch 4/20\n",
      "1134/1134 [==============================] - 73s 64ms/step - loss: 0.5164 - main_out_loss: 0.3911 - aux_out_loss: 0.6263 - main_out_accuracy: 0.8191 - aux_out_accuracy: 0.6797 - val_loss: 0.7558 - val_main_out_loss: 0.6273 - val_aux_out_loss: 0.6427 - val_main_out_accuracy: 0.7158 - val_aux_out_accuracy: 0.62050.3897 - aux_out_loss: 0.6270 -  - ETA: 9s - loss: 0.5154 - main_out_loss: 0.3900 - aux_out_loss: 0.6269 - main_out_accuracy: 0.8195 - a - ETA: 7s - loss: 0.5155 - main_out_loss: 0.3902 - aux_out_loss: 0.6268 - main_out_accuracy: 0.8194 - aux_out_ac - ETA: 6s - loss: 0.5156 - main_out_loss: 0.3903 - ETA: 1s - loss: 0.5162 - main_out_loss: 0.3909 - aux_out_loss: 0.6264 - main_out_accuracy: 0.8191 - aux_out_accuracy: 0.67 - ETA: 1s - loss: 0.5162 - main_out_loss: 0.3909 - aux_out_loss: 0.6264 - main_out_accuracy: 0.8191 - aux_out_accuracy - ETA: 1s - loss: 0.5162 - main_out_loss: 0.3910 - aux_out_loss: 0.6264 - main_out_accuracy: 0.8191 - aux_ou\n",
      "Epoch 5/20\n",
      "1134/1134 [==============================] - 72s 64ms/step - loss: 0.4630 - main_out_loss: 0.3425 - aux_out_loss: 0.6024 - main_out_accuracy: 0.8496 - aux_out_accuracy: 0.7072 - val_loss: 0.7751 - val_main_out_loss: 0.6477 - val_aux_out_loss: 0.6369 - val_main_out_accuracy: 0.7101 - val_aux_out_accuracy: 0.63660.8627 - aux_out_ - ETA: 1:02 - loss: 0.4583 - main_out_loss: 0.3364 - aux_out_loss: 0.6091 - main_out_accuracy: 0.8619 - ETA: 6s - loss: 0.4621 - main_out_loss: 0.3416 - aux_out_loss: 0.6028 - main_out - ETA: 3s - loss: 0.4626 - main_out_loss: 0.3420 - aux_out_loss: 0.6026 - mai\n",
      "Epoch 6/20\n",
      "1134/1134 [==============================] - 72s 64ms/step - loss: 0.4111 - main_out_loss: 0.2953 - aux_out_loss: 0.5792 - main_out_accuracy: 0.8734 - aux_out_accuracy: 0.7275 - val_loss: 0.9026 - val_main_out_loss: 0.7750 - val_aux_out_loss: 0.6383 - val_main_out_accuracy: 0.6924 - val_aux_out_accuracy: 0.62732852 - aux_out_loss: 0.5802 - main_out_accuracy: 0.8802 - aux_out_ac - ETA: 39s - loss: 0.4021 - main_out_loss: 0. - ETA: 7s - loss: 0.4092 - main_out_l - ETA: 2s - loss: 0.4106 - main_out_loss: 0.2947 - aux_out_loss: 0.5793 - main_out_accuracy: 0.8737 - aux_out_accurac - ETA: 1s - loss: 0.4107 - main_out_loss: 0.2948 - aux_out_loss: 0.5793 - main_out_accuracy: 0.873\n",
      "Epoch 7/20\n",
      "1134/1134 [==============================] - 74s 65ms/step - loss: 0.3675 - main_out_loss: 0.2556 - aux_out_loss: 0.5594 - main_out_accuracy: 0.8937 - aux_out_accuracy: 0.7375 - val_loss: 0.8957 - val_main_out_loss: 0.7697 - val_aux_out_loss: 0.6299 - val_main_out_accuracy: 0.7055 - val_aux_out_accuracy: 0.636756s - loss: 0.3602 - main_out_loss: 0.2478 - aux_out_loss: 0.5623 - main_out_accurac - ETA: 54s - loss: 0.3601 - main_out_loss: 0.2477 - aux_out_loss: 0.5622 - main_out_accuracy: 0.9002 - aux_ - ETA: 53s - loss: 0.3601 - main_out_loss: 0.2476 - aux_out_loss: 0.5621 - ETA: 6s - loss: 0.3662 - main_out_loss: 0.2543 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8945 - aux_out_accu - ETA: 5s - loss: 0.3664 - main_out_loss: 0.2545 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8944 - aux_out_ - ETA: 4s - loss: 0.3666 - main_out_loss: 0.2547 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8942 - aux_out_accuracy: 0.73 - ETA: 4s - loss: 0.3666 - main_out_loss: 0.2547 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8942 - aux_out_accurac - ETA: 3s - loss: 0.3667 - main_out_loss: 0.2548 - aux_out_loss: 0.559 - ETA: 0s - loss: 0.3674 - main_out_loss: 0.2556 - aux_out_loss: 0.5594 - main_out_accuracy: 0.8937 - aux_out_accuracy: \n",
      "Using the dummy classifier (assuming all posts are less than or equal to the median), the accuracy is: \n",
      "0.5047719867012919\n",
      "The accuracy of the model on the validation set is: \n",
      "0.7247766852378845\n",
      "The accuracy of the model on the test set is: \n",
      "0.7211379408836365\n"
     ]
    }
   ],
   "source": [
    "model, accuracies, word_tokenizer, df = CreateNeuralNets.buildnets(['wallstreetbets'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066db0d",
   "metadata": {},
   "source": [
    "## Predicted popularity as a time series\n",
    "\n",
    "We now show how the predicted popularity of a post depends on the day on which it was posted. \n",
    "We plot the prediction for the same title, \"GME GME GME GME GME GME\", as if it were posted at noon each day. \n",
    "It is interesting to note that the variance seems to decrease after the GameStop short squeeze of early 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bc89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"GME GME GME GME GME GME\"\n",
    "CreateNeuralNets.timeseries(df, text, model, word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb01c8",
   "metadata": {},
   "source": [
    "This will produce a picture like the following:\n",
    "![title](../Data/subreddit_WallStreetBets/6_GME.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28ea7f",
   "metadata": {},
   "source": [
    "## Workshopping example\n",
    "Here we start with a potential title (to be posted at noon on April 1, 2021) and attempt to improve it based on the model's prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7da33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the date information for April 1, 2021. \n",
    "#Note we normalize so the earliest year in our data set (2020) \n",
    "#and the earliest day of the year correspond to the number 0\n",
    "input_hour = np.array([12])\n",
    "input_dayofweek = np.array([3])\n",
    "input_minute = np.array([0])\n",
    "input_dayofyear = np.array([91])\n",
    "input_year = np.array([0])\n",
    "input_info=[input_hour,input_dayofweek, input_minute, input_dayofyear, input_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69227a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of potential titles, predict the success of each one\n",
    "def CheckPopularity(potential_titles):\n",
    "    for title in potential_titles:\n",
    "        print(model.predict([CreateNeuralNets.encode_text(title,word_tokenizer)] + input_info)[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5968dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_titles = [\"Buy TSLA\", \"Buy TSLA! I like the stock\", \"Buy TSLA! Elon likes the stock\",\n",
    "                    \"TSLA is the next GME. Elon likes the stock\", \n",
    "                    \"TSLA is the next GME. To the moon! Elon likes the stock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a9af5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9536921\n",
      "0.957647\n",
      "0.9620316\n",
      "0.98298347\n",
      "0.983858\n"
     ]
    }
   ],
   "source": [
    "CheckPopularity(potential_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfee3f",
   "metadata": {},
   "source": [
    "We see that the predicted popularity increases after each change we make. \n",
    "\n",
    "Disclaimer: we are investigating a last-minute issue. These probabilities are higher than expected from earlier experimentation, and there is the possibility of a bug in our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c099a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c42fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0786fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bc2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a8f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164aa72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28db60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08954492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad6cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d7f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
