{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80601a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lnajt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "os.chdir(\"../Automated/\")\n",
    "from DataGathering import RedditScraper\n",
    "from ChangePointAnalysis import ChangePointAnalysis\n",
    "from NeuralNets import CreateNeuralNets\n",
    "\n",
    "\n",
    "subreddit = \"WallStreetBets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d87b64",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "For analyzing wallstreetbets data, we recommend downloading full.csv from [url] and putting it in ../Data/subreddit_wallstreetbets.\n",
    "\n",
    "If you want to scrape a different subreddit, you can use the following file. You will need API.env with appropriate credentials in /Automated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc661bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = dt.datetime(2020, 1, 1)\n",
    "end =  dt.datetime(2020, 1, 30)\n",
    "\n",
    "if not os.path.exists(f\"../Data/subreddit_{subreddit}/full.csv\"):\n",
    "    print(\"Did not find scraped data, scraping.\")\n",
    "\n",
    "    RedditScraper.scrape_data(subreddits = [subreddit], start = start, end = end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87478f4",
   "metadata": {},
   "source": [
    "# Change Point Analysis\n",
    "\n",
    "The next cell will open full.csv , compute the words that are among the top daily_words most popular words on any day, and then run the change point analysis model on each of them.\n",
    "\n",
    "\n",
    "The first time this is a run, a cleaned up version of the dataframe will be created for ease of processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e579ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the changepoints:\n",
      "working on  WallStreetBets\n",
      "['log', 'kodak', 'short', 'mnmd', 'monday', 'snap', 'month', 'wsb', 'next', 'slv', 'questrade', 'rh', 'gold', 'iran', 'mascot', 'ev', 'covid', 'rkt', 'dd', 'fed', 'biden', 'may', 'apes', 'nikola', 'robinhood', 'nkla', 'elon', 'options', 'buy', 'coronavirus', 'clov', 'msft', 'azn', 'year', 'july', 'suicide', 'citron', 'trevor', 'merry', 'gme', 'detroit', 'plug', 'debate', 'mt', 'tomorrow', 'wkhs', 'test', 'bezos', 'new', 'trading', 'spy', 'two', 'one', 'twitter', 'election', 'get', 'christmas', 'tendies', 'sndl', 'tsla', 'puts', 'oil', 'sos', 'virus', 'war', 'moon', 'spce', 'k', 'week', 'today', 'hold', 'dogecoin', 'uso', 'amd', 'trump', 'earnings', 'mvis', 'jpow', 'im', 'amc', 'bear', 'currency', 'day', 'market', 'pltr', 'tesla', 'kodk', 'stock', 'gang', 'go', 'stocks', 'bb', 'money', 'closed', 'nio', 'coin', 'prpl', 'calls', 'like', 'fortnite'] pop_words\n",
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\n",
      "working on log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\\ChangePointAnalysis\\BayesianMethods.py:134: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(steps, tune=tune, step = step)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [delta]\n",
      ">Metropolis: [change_point_beta]\n",
      ">Metropolis: [tau_beta]\n",
      ">Metropolis: [beta_2]\n",
      ">Metropolis: [alpha_2]\n",
      ">Metropolis: [beta_1]\n",
      ">Metropolis: [alpha_1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='140000' class='' max='140000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [140000/140000 05:55<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 5_000 tune and 30_000 draw iterations (20_000 + 120_000 draws total) took 395 seconds.\n",
      "The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change Point Guess 0.25\n",
      "{'change_point_confidence': [0.25], 'mus': [(0.009015622219098773, 0.011524003066574464)], 'mu_diff': [0.0025083808474756913], 'tau_map': ['2020-01-01'], 'tau_std': [137.09623688628048], 'entropy': [5.218872264011381], 'change_point_guess': [0.25]}\n",
      "appending\n",
      "working on kodak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lnajt\\Documents\\GitHub\\ErdosInstitute\\ErdosInstitute-SIG_Project\\Automated\\ChangePointAnalysis\\BayesianMethods.py:134: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(steps, tune=tune, step = step)\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [delta]\n",
      ">Metropolis: [change_point_beta]\n",
      ">Metropolis: [tau_beta]\n",
      ">Metropolis: [beta_2]\n",
      ">Metropolis: [alpha_2]\n",
      ">Metropolis: [beta_1]\n",
      ">Metropolis: [alpha_1]\n"
     ]
    }
   ],
   "source": [
    "up_to = 1 # Only calculate change points for up_to of the popular words. Set to None to do all of them.\n",
    "daily_words = 2 # Get the daily_words most popular posts on each day.\n",
    "\n",
    "\n",
    "# Compute the changepoints\n",
    "ChangePointAnalysis.changepointanalysis([subreddit], up_to = up_to, daily_words = daily_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f31c07",
   "metadata": {},
   "source": [
    "After running, these files will in ../Data/subreddit_subreddit/Changepoints/Metropolis_30000Draws_5000Tune\n",
    "\n",
    "(The final folder corresponds to the parameters of the Markov chain used by pymc3 for the inference.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceea485",
   "metadata": {},
   "source": [
    "For instance: \n",
    "    \n",
    "![title](../Data/subreddit_WallStreetBets/Changepoints/Metropolis_30000Draws_5000Tune/ChangePoint_virus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aae378",
   "metadata": {},
   "source": [
    "### Brief explanation of how this works:\n",
    "\n",
    "The Bayesian model is as follows:\n",
    "\n",
    "1. A coin is flipped with probability p.\n",
    "2. If the coin comes up heads, then there is a change point. Otherwise, there is no change point.\n",
    "3. It is assumed that the frequency random variable consists of independent draws from a beta distribution. If the coin decided there would be no change point, it is the same beta distribution at all times. Otherwise, it is a different beta on the different sides of the change points.\n",
    "\n",
    "The posterior distribution of p is the models confidence that there is a change point, and the posterior distribution of tau represents its guess about when it occured.\n",
    "\n",
    "Of course, this is not a realistic picture of the process; the independence of the different draws from the betas is especially unlike the data. However, it appears to be good enough to discover change points.\n",
    "\n",
    "As currently written, it only handles one change point, however this can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ec0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92333afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee28b8c",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "\n",
    "The following code will train a neural net that predicts, given a submission's title text and time of posting, whether that submission's score will be above the median. \n",
    "\n",
    "We use pre-trained GloVe word embeddings in order to convert the title text into a vector that can be used in the neural net. These word embeddings are tuned along with the model parameters as the model is being trained. \n",
    "\n",
    "This technique and the neural net's architecture are taken from a blog post of Max Woolf, https://minimaxir.com/2017/06/reddit-deep-learning/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, accuracies, word_tokenizer, df = CreateNeuralNets.buildnets([subreddit])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066db0d",
   "metadata": {},
   "source": [
    "## Predicted popularity as a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"GME GME GME GME GME GME\"\n",
    "CreateNeuralNets.timeseries(df, text, model, word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb01c8",
   "metadata": {},
   "source": [
    "This will produce a picture like the following:\n",
    "![title](../Data/subreddit_WallStreetBets/6_GME.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28ea7f",
   "metadata": {},
   "source": [
    "## Workshopping example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da33a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
