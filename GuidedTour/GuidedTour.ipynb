{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80601a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "os.chdir(\"../Automated/\")\n",
    "from DataGathering import RedditScraper\n",
    "from ChangePointAnalysis import ChangePointAnalysis\n",
    "from NeuralNets import CreateNeuralNets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "subreddit = \"WallStreetBets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ed65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01d87b64",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "For analyzing wallstreetbets data, we recommend downloading full.csv from [url] and putting it in ../Data/subreddit_wallstreetbets.\n",
    "\n",
    "If you want to scrape a different subreddit, you can use the following file. You will need API.env with appropriate credentials in /Automated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc661bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = dt.datetime(2020, 1, 1)\n",
    "end =  dt.datetime(2020, 1, 30)\n",
    "\n",
    "if not os.path.exists(f\"../Data/subreddit_{subreddit}/full.csv\"):\n",
    "    print(\"Did not find scraped data, scraping.\")\n",
    "\n",
    "    RedditScraper.scrape_data(subreddits = [subreddit], start = start, end = end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87478f4",
   "metadata": {},
   "source": [
    "# Change Point Analysis\n",
    "\n",
    "The next cell will open full.csv , compute the words that are among the top daily_words most popular words on any day, and then run the change point analysis model on each of them.\n",
    "\n",
    "\n",
    "The first time this is a run, a cleaned up version of the dataframe will be created for ease of processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e579ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "up_to = None # Only calculate change points for up_to of the popular words. Set to None to do all of them.\n",
    "daily_words = 1 # Get the daily_words most popular posts on each day.\n",
    "\n",
    "\n",
    "# Compute the changepoints\n",
    "# ChangePointAnalysis.changepointanalysis([subreddit], up_to = up_to, daily_words = daily_words)\n",
    "\n",
    "# The output has been commented out because it is very long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f31c07",
   "metadata": {},
   "source": [
    "After running, these files will in ../Data/subreddit_subreddit/Changepoints/Metropolis_30000Draws_5000Tune\n",
    "\n",
    "The final folder name corresponds to the parameters of the Markov chain used by pymc3 for the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aae378",
   "metadata": {},
   "source": [
    "## Organizing the changepoints\n",
    "\n",
    "A table of the keywords considered with parameters estimated by the model is stored in :  ../Data/subreddit_{subreddit}/Changepoints/results.csv\n",
    "\n",
    "You can then sort through these to find the keywords the model detected a change in.\n",
    "\n",
    "There are two key parameters:\n",
    "\n",
    "change_point_confidence (also denoted p): the models belief that there was a changepoint. p = 1 indicates yes, p = 0 indicates no.\n",
    "\n",
    "mu_diff : a measurement of the size of the changepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15982c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "26    0.032728\n33    0.038319\n34    0.033941\n42   -0.033556\n49    0.087074\n53    0.046182\nName: mu_diff, dtype: float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-d89586d7122a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"Unnamed: 0\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"keyword\"\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_point_confidence\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu_diff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m.03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   5453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5454\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5455\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5457\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 26    0.032728\n33    0.038319\n34    0.033941\n42   -0.033556\n49    0.087074\n53    0.046182\nName: mu_diff, dtype: float64"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f\"../Data/subreddit_{subreddit}/Changepoints/results.csv\")\n",
    "results = results.rename( columns = {\"Unnamed: 0\" : \"keyword\" })\n",
    "filtered = results[(results.change_point_confidence == 1) & (results.mu_diff.apply(lambda x : np.abs(x)) > .03)]\n",
    "filtered = filtered.sort_values(by = filtered.mu_diff)\n",
    "for row in filtered.iterrows():\n",
    "\n",
    "    word = row[1][\"keyword\"]\n",
    "    print(row[1][\"keyword\"], \" Change point confidence: \", row[1][\"change_point_confidence\"], \" Change point magnitude: \", row[1][\"mu_diff\"])\n",
    "\n",
    "    img = mpimg.imread(f'../Data/subreddit_WallStreetBets/Changepoints/Metropolis_30000Draws_5000Tune/ChangePoint_{word}.png')\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3a4fe",
   "metadata": {},
   "source": [
    "## Warning: \n",
    "\n",
    "Sometimes change point confidence alone is not enough. For instance, it was very confident (p = 1) that there was a changepoint in the following, although it wouldn't be reported as a change point because mu_2 - mu_1 was very small (~.003):\n",
    "\n",
    "![title](../Data/subreddit_WallStreetBets/Changepoints/Metropolis_30000Draws_5000Tune/ChangePoint_new.png)\n",
    "\n",
    "## Brief explanation of how this model works:\n",
    "\n",
    "The Bayesian model is as follows:\n",
    "\n",
    "1. A coin is flipped with probability p.\n",
    "2. If the coin comes up heads, then there is a change point. Otherwise, there is no change point.\n",
    "3. It is assumed that the frequency random variable consists of independent draws from a beta distribution. If the coin decided there would be no change point, it is the same beta distribution at all times. Otherwise, it is a different beta on the different sides of the change points.\n",
    "\n",
    "The posterior distribution of p is the models confidence that there is a change point, and the posterior distribution of tau represents its guess about when it occured.\n",
    "\n",
    "The variable mu_1 represents the mean of the beta distribution before the change point, and mu_2 represents the mean of the beta distribution after the changepoint.\n",
    "\n",
    "Of course, this is not a realistic picture of the process; the independence of the different draws from the betas is especially unlike the data. However, it appears to be good enough to discover change points, especially when p and mu_2 - mu_1 are used together.\n",
    "\n",
    "As currently written, it only handles one change point, however this can be improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee28b8c",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "\n",
    "The following code will train a neural net that predicts, given a submission's title text and time of posting, whether that submission's score will be above the median. \n",
    "\n",
    "We use pre-trained GloVe word embeddings in order to convert the title text into a vector that can be used in the neural net. These word embeddings are tuned along with the model parameters as the model is being trained. \n",
    "\n",
    "This technique and the neural net's architecture are taken from a blog post of Max Woolf, https://minimaxir.com/2017/06/reddit-deep-learning/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951f5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Post Classification Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (26,32,45,79,82,87,116,117,118,121,122,123,124) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1134/1134 [==============================] - 98s 58ms/step - loss: 0.7454 - main_out_loss: 0.6063 - aux_out_loss: 0.6957 - main_out_accuracy: 0.6723 - aux_out_accuracy: 0.5042 - val_loss: 0.6840 - val_main_out_loss: 0.5480 - val_aux_out_loss: 0.6802 - val_main_out_accuracy: 0.7165 - val_aux_out_accuracy: 0.5756\n",
      "Epoch 2/20\n",
      "1134/1134 [==============================] - 66s 58ms/step - loss: 0.6384 - main_out_loss: 0.5037 - aux_out_loss: 0.6739 - main_out_accuracy: 0.7492 - aux_out_accuracy: 0.5958 - val_loss: 0.6675 - val_main_out_loss: 0.5346 - val_aux_out_loss: 0.6647 - val_main_out_accuracy: 0.7248 - val_aux_out_accuracy: 0.6091\n",
      "Epoch 3/20\n",
      "1134/1134 [==============================] - 81s 72ms/step - loss: 0.5735 - main_out_loss: 0.4437 - aux_out_loss: 0.6494 - main_out_accuracy: 0.7905 - aux_out_accuracy: 0.6490 - val_loss: 0.6856 - val_main_out_loss: 0.5551 - val_aux_out_loss: 0.6523 - val_main_out_accuracy: 0.7222 - val_aux_out_accuracy: 0.6274- loss: 0.5634 - main_ou - ETA: 54s - loss: 0.5642 - main_out_loss: 0.4337 - aux_out_loss: 0.6525 - main_out_accuracy: 0.8024 -  - ETA: 35s - loss: 0.56 - ETA: 19s - loss: 0.5715 - main_out_ - ETA: 5s - loss: 0.5729 - main_out_loss: 0.4429 - aux_out_loss: 0.6497 - main_out_accuracy: 0.7912 - au - ETA: 4s - loss: 0.5730 - main_out_loss: 0.4431 - aux_out_loss: 0.6496 - main_out_accuracy: 0.791 - ETA: 2s - loss: 0.5733 - main_out_loss: 0.4434 - aux_out_loss: 0.6495 - main_out_accuracy: 0.790 - ETA: 0s - loss: 0.5735 - main_out_loss: 0.4436 - aux_out_loss: 0.6494 - main_out_accuracy: 0.7906 - aux_out_accurac\n",
      "Epoch 4/20\n",
      "1134/1134 [==============================] - 73s 64ms/step - loss: 0.5164 - main_out_loss: 0.3911 - aux_out_loss: 0.6263 - main_out_accuracy: 0.8191 - aux_out_accuracy: 0.6797 - val_loss: 0.7558 - val_main_out_loss: 0.6273 - val_aux_out_loss: 0.6427 - val_main_out_accuracy: 0.7158 - val_aux_out_accuracy: 0.62050.3897 - aux_out_loss: 0.6270 -  - ETA: 9s - loss: 0.5154 - main_out_loss: 0.3900 - aux_out_loss: 0.6269 - main_out_accuracy: 0.8195 - a - ETA: 7s - loss: 0.5155 - main_out_loss: 0.3902 - aux_out_loss: 0.6268 - main_out_accuracy: 0.8194 - aux_out_ac - ETA: 6s - loss: 0.5156 - main_out_loss: 0.3903 - ETA: 1s - loss: 0.5162 - main_out_loss: 0.3909 - aux_out_loss: 0.6264 - main_out_accuracy: 0.8191 - aux_out_accuracy: 0.67 - ETA: 1s - loss: 0.5162 - main_out_loss: 0.3909 - aux_out_loss: 0.6264 - main_out_accuracy: 0.8191 - aux_out_accuracy - ETA: 1s - loss: 0.5162 - main_out_loss: 0.3910 - aux_out_loss: 0.6264 - main_out_accuracy: 0.8191 - aux_ou\n",
      "Epoch 5/20\n",
      "1134/1134 [==============================] - 72s 64ms/step - loss: 0.4630 - main_out_loss: 0.3425 - aux_out_loss: 0.6024 - main_out_accuracy: 0.8496 - aux_out_accuracy: 0.7072 - val_loss: 0.7751 - val_main_out_loss: 0.6477 - val_aux_out_loss: 0.6369 - val_main_out_accuracy: 0.7101 - val_aux_out_accuracy: 0.63660.8627 - aux_out_ - ETA: 1:02 - loss: 0.4583 - main_out_loss: 0.3364 - aux_out_loss: 0.6091 - main_out_accuracy: 0.8619 - ETA: 6s - loss: 0.4621 - main_out_loss: 0.3416 - aux_out_loss: 0.6028 - main_out - ETA: 3s - loss: 0.4626 - main_out_loss: 0.3420 - aux_out_loss: 0.6026 - mai\n",
      "Epoch 6/20\n",
      "1134/1134 [==============================] - 72s 64ms/step - loss: 0.4111 - main_out_loss: 0.2953 - aux_out_loss: 0.5792 - main_out_accuracy: 0.8734 - aux_out_accuracy: 0.7275 - val_loss: 0.9026 - val_main_out_loss: 0.7750 - val_aux_out_loss: 0.6383 - val_main_out_accuracy: 0.6924 - val_aux_out_accuracy: 0.62732852 - aux_out_loss: 0.5802 - main_out_accuracy: 0.8802 - aux_out_ac - ETA: 39s - loss: 0.4021 - main_out_loss: 0. - ETA: 7s - loss: 0.4092 - main_out_l - ETA: 2s - loss: 0.4106 - main_out_loss: 0.2947 - aux_out_loss: 0.5793 - main_out_accuracy: 0.8737 - aux_out_accurac - ETA: 1s - loss: 0.4107 - main_out_loss: 0.2948 - aux_out_loss: 0.5793 - main_out_accuracy: 0.873\n",
      "Epoch 7/20\n",
      "1134/1134 [==============================] - 74s 65ms/step - loss: 0.3675 - main_out_loss: 0.2556 - aux_out_loss: 0.5594 - main_out_accuracy: 0.8937 - aux_out_accuracy: 0.7375 - val_loss: 0.8957 - val_main_out_loss: 0.7697 - val_aux_out_loss: 0.6299 - val_main_out_accuracy: 0.7055 - val_aux_out_accuracy: 0.636756s - loss: 0.3602 - main_out_loss: 0.2478 - aux_out_loss: 0.5623 - main_out_accurac - ETA: 54s - loss: 0.3601 - main_out_loss: 0.2477 - aux_out_loss: 0.5622 - main_out_accuracy: 0.9002 - aux_ - ETA: 53s - loss: 0.3601 - main_out_loss: 0.2476 - aux_out_loss: 0.5621 - ETA: 6s - loss: 0.3662 - main_out_loss: 0.2543 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8945 - aux_out_accu - ETA: 5s - loss: 0.3664 - main_out_loss: 0.2545 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8944 - aux_out_ - ETA: 4s - loss: 0.3666 - main_out_loss: 0.2547 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8942 - aux_out_accuracy: 0.73 - ETA: 4s - loss: 0.3666 - main_out_loss: 0.2547 - aux_out_loss: 0.5595 - main_out_accuracy: 0.8942 - aux_out_accurac - ETA: 3s - loss: 0.3667 - main_out_loss: 0.2548 - aux_out_loss: 0.559 - ETA: 0s - loss: 0.3674 - main_out_loss: 0.2556 - aux_out_loss: 0.5594 - main_out_accuracy: 0.8937 - aux_out_accuracy: \n",
      "Using the dummy classifier (assuming all posts are less than or equal to the median), the accuracy is: \n",
      "0.5047719867012919\n",
      "The accuracy of the model on the validation set is: \n",
      "0.7247766852378845\n",
      "The accuracy of the model on the test set is: \n",
      "0.7211379408836365\n"
     ]
    }
   ],
   "source": [
    "model, accuracies, word_tokenizer, df = CreateNeuralNets.buildnets(['wallstreetbets'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066db0d",
   "metadata": {},
   "source": [
    "## Predicted popularity as a time series\n",
    "\n",
    "We now show how the predicted popularity of a post depends on the day on which it was posted. \n",
    "We plot the prediction for the same title, \"GME GME GME GME GME GME\", as if it were posted at noon each day. \n",
    "It is interesting to note that the variance seems to decrease after the GameStop short squeeze of early 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bc89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"GME GME GME GME GME GME\"\n",
    "CreateNeuralNets.timeseries(df, text, model, word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb01c8",
   "metadata": {},
   "source": [
    "This will produce a picture like the following:\n",
    "![title](../Data/subreddit_WallStreetBets/6_GME.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28ea7f",
   "metadata": {},
   "source": [
    "## Workshopping example\n",
    "Here we start with a potential title (to be posted at noon on April 1, 2021) and attempt to improve it based on the model's prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7da33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the date information for April 1, 2021. \n",
    "#Note we normalize so the earliest year in our data set (2020) \n",
    "#and the earliest day of the year correspond to the number 0\n",
    "input_hour = np.array([12])\n",
    "input_dayofweek = np.array([3])\n",
    "input_minute = np.array([0])\n",
    "input_dayofyear = np.array([91])\n",
    "input_year = np.array([0])\n",
    "input_info=[input_hour,input_dayofweek, input_minute, input_dayofyear, input_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c69227a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a list of potential titles, predict the success of each one\n",
    "def CheckPopularity(potential_titles):\n",
    "    for title in potential_titles:\n",
    "        print(model.predict([CreateNeuralNets.encode_text(title,word_tokenizer)] + input_info)[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5968dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_titles = [\"Buy TSLA\", \"Buy TSLA! I like the stock\", \"Buy TSLA! Elon likes the stock\",\n",
    "                    \"TSLA is the next GME. Elon likes the stock\", \n",
    "                    \"TSLA is the next GME. To the moon! Elon likes the stock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a9af5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9536921\n",
      "0.957647\n",
      "0.9620316\n",
      "0.98298347\n",
      "0.983858\n"
     ]
    }
   ],
   "source": [
    "CheckPopularity(potential_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfee3f",
   "metadata": {},
   "source": [
    "We see that the predicted popularity increases after each change we make. \n",
    "\n",
    "Disclaimer: we are investigating a last-minute issue. These probabilities are higher than expected from earlier experimentation, and there is the possibility of a bug in our code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
