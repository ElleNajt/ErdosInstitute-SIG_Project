{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Success of a Reddit Submission with Deep Learning and Keras\n",
    "\n",
    "2016-06-26\n",
    "\n",
    "by Max Woolf ([@minimaxir](http://minimaxir.com))\n",
    "\n",
    "*Max's open-source projects are supported by his [Patreon](https://www.patreon.com/minimaxir). If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "BigQuery used to get data:\n",
    "\n",
    "```sql\n",
    "#standardSQL \n",
    "SELECT id, title,\n",
    "  CAST(FORMAT_TIMESTAMP('%H', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS hour,\n",
    "  CAST(FORMAT_TIMESTAMP('%M', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS minute,\n",
    "  CAST(FORMAT_TIMESTAMP('%w', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS dayofweek,\n",
    "  CAST(FORMAT_TIMESTAMP('%j', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS dayofyear,\n",
    "  IF(PERCENT_RANK() OVER (ORDER BY score ASC) >= 0.50, 1, 0) as is_top_submission\n",
    "  FROM `fh-bigquery.reddit_posts.*`\n",
    "  WHERE (_TABLE_SUFFIX BETWEEN '2017_01' AND '2017_04')\n",
    "  AND subreddit = 'AskReddit'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from random import random, sample, seed\n",
    "\n",
    "data_path = '/Volumes/Extreme 510/Data/askreddit_data_timings.csv'\n",
    "embeddings_path = '/Volumes/Extreme 510/Data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "hours = []\n",
    "minutes = []\n",
    "dayofweeks = []\n",
    "dayofyears = []\n",
    "is_top_submission = []\n",
    "\n",
    "with open(data_path, 'r', encoding=\"latin1\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for submission in reader:\n",
    "        titles.append(submission['title'])\n",
    "        hours.append(submission['hour'])\n",
    "        minutes.append(submission['minute'])\n",
    "        dayofweeks.append(submission['dayofweek'])\n",
    "        dayofyears.append(submission['dayofyear'])\n",
    "        is_top_submission.append(submission['is_top_submission'])\n",
    "            \n",
    "titles = np.array(titles)\n",
    "hours = np.array(hours, dtype=int)\n",
    "minutes = np.array(minutes, dtype=int)\n",
    "dayofweeks = np.array(dayofweeks, dtype=int)\n",
    "dayofyears = np.array(dayofyears, dtype=int)\n",
    "is_top_submission = np.array(is_top_submission, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['People who have been cheated on: how did you find out?'\n",
      " \"What is the biggest display of confidence/charisma you've ever seen?\"]\n",
      "(976538,)\n",
      "[ 1 16]\n",
      "[10 48]\n",
      "[3 3]\n",
      "[46 46]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print(titles[0:2])\n",
    "print(titles.shape)\n",
    "print(hours[0:2])\n",
    "print(minutes[0:2])\n",
    "print(dayofweeks[0:2])\n",
    "print(dayofyears[0:2])\n",
    "print(is_top_submission[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64075949937432031"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.mean(is_top_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The No-Information Rate is 64% (i.e. simply say all AskReddit submissions are terrible), so any model trained must do better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process /r/AskReddit Submission Title Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 77432, 'who': 80438, 'have': 120730, 'been': 24255, 'cheated': 946, 'on': 87077, 'how': 1\n",
      "{'you': 1, 'what': 2, 'the': 3, 'to': 4, 'a': 5, 'of': 6, 'your': 7, 'is': 8, 'do': 9, 'and': 10, 'i\n",
      "98770\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "max_features = 40000\n",
    "\n",
    "word_tokenizer = Tokenizer(max_features)\n",
    "word_tokenizer.fit_on_texts(titles)\n",
    "\n",
    "print(str(word_tokenizer.word_counts)[0:100])\n",
    "print(str(word_tokenizer.word_index)[0:100])\n",
    "print(len(word_tokenizer.word_counts))   # true word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 26, 17, 75, 1222, 24, 15, 33, 1, 119, 55]\n"
     ]
    }
   ],
   "source": [
    "titles_tf = word_tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "print(titles_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0   27   26   17   75 1222   24\n",
      "   15   33    1  119   55]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "titles_tf = sequence.pad_sequences(titles_tf, maxlen=maxlen)\n",
    "\n",
    "print(titles_tf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Pretrained Embeddings\n",
    "\n",
    "Adapted from [the official keras tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "\n",
    "Use pretrained GloVe embeddings to both give Embeddings training a good start, and to account for words that might be present in the test set but not in the training set.\n",
    "\n",
    "First, load the 50D embeddings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.09190000e-03   3.33240000e-01   3.57430000e-01  -5.40410000e-01\n",
      "   8.20320000e-01  -4.93910000e-01  -3.25880000e-01   1.99720000e-03\n",
      "  -2.38290000e-01   3.55540000e-01  -6.06550000e-01   9.89320000e-01\n",
      "  -2.17860000e-01   1.12360000e-01   1.14940000e+00   7.32840000e-01\n",
      "   5.11820000e-01   2.92870000e-01   2.83880000e-01  -1.35900000e+00\n",
      "  -3.79510000e-01   5.09430000e-01   7.07100000e-01   6.29410000e-01\n",
      "   1.05340000e+00  -2.17560000e+00  -1.32040000e+00   4.00010000e-01\n",
      "   1.57410000e+00  -1.66000000e+00   3.77210000e+00   8.69490000e-01\n",
      "  -8.04390000e-01   1.83900000e-01  -3.43320000e-01   1.07140000e-02\n",
      "   2.39690000e-01   6.67480000e-02   7.01170000e-01  -7.37020000e-01\n",
      "   2.08770000e-01   1.15640000e-01  -1.51900000e-01   8.59080000e-01\n",
      "   2.26200000e-01   1.65190000e-01   3.63090000e-01  -4.56970000e-01\n",
      "  -4.89690000e-02   1.13160000e+00]\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = {}\n",
    "\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = np.array(line_split[1:], dtype=float)\n",
    "        word = line_split[0]\n",
    "        embedding_vectors[word] = vec\n",
    "        \n",
    "print(embedding_vectors['you'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights matrix as zeroes, then replace the corresponding index of the weights matrix with the index of the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [ -1.09190000e-03   3.33240000e-01   3.57430000e-01  -5.40410000e-01\n",
      "    8.20320000e-01  -4.93910000e-01  -3.25880000e-01   1.99720000e-03\n",
      "   -2.38290000e-01   3.55540000e-01  -6.06550000e-01   9.89320000e-01\n",
      "   -2.17860000e-01   1.12360000e-01   1.14940000e+00   7.32840000e-01\n",
      "    5.11820000e-01   2.92870000e-01   2.83880000e-01  -1.35900000e+00\n",
      "   -3.79510000e-01   5.09430000e-01   7.07100000e-01   6.29410000e-01\n",
      "    1.05340000e+00  -2.17560000e+00  -1.32040000e+00   4.00010000e-01\n",
      "    1.57410000e+00  -1.66000000e+00   3.77210000e+00   8.69490000e-01\n",
      "   -8.04390000e-01   1.83900000e-01  -3.43320000e-01   1.07140000e-02\n",
      "    2.39690000e-01   6.67480000e-02   7.01170000e-01  -7.37020000e-01\n",
      "    2.08770000e-01   1.15640000e-01  -1.51900000e-01   8.59080000e-01\n",
      "    2.26200000e-01   1.65190000e-01   3.63090000e-01  -4.56970000e-01\n",
      "   -4.89690000e-02   1.13160000e+00]]\n"
     ]
    }
   ],
   "source": [
    "weights_matrix = np.zeros((max_features + 1, 50))\n",
    "\n",
    "for word, i in word_tokenizer.word_index.items():\n",
    "\n",
    "    embedding_vector = embedding_vectors.get(word)\n",
    "    if embedding_vector is not None and i <= max_features:\n",
    "        weights_matrix[i] = embedding_vector\n",
    "\n",
    "# index 0 vector should be all zeroes, index 1 vector should be the same one as above\n",
    "print(weights_matrix[0:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Other Metadata\n",
    "\n",
    "All metadata must be zero-indexed integers.\n",
    "\n",
    "* `hours` in the correct format. (`0` = 12AM EST, `23` = 11PM EST)\n",
    "* `dayofweeks` is in the correct format (`0` = Sunday, `6` = Saturday)\n",
    "* `minutes` is in the correct format verbatim.\n",
    "* `dayofyears` is 1-indexed, so must subtract 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 45  45  44 104  47  46 115 115  57   1]\n"
     ]
    }
   ],
   "source": [
    "dayofyears_tf = dayofyears - 1\n",
    "\n",
    "print(dayofyears_tf[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "Use Keras's functional API to build a branching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation\n",
    "from keras.layers.core import Masking, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Branch\n",
    "\n",
    "Encode the text using a mock fasttext approach. Use `weights_matrix` derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_input = Input(shape=(maxlen,), name='titles_input')\n",
    "titles_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(titles_input)\n",
    "titles_pooling = GlobalAveragePooling1D()(titles_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an auxillary output to regularize the text component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_output = Dense(1, activation='sigmoid', name='aux_out')(titles_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Branch\n",
    "\n",
    "Each metadata variable gets its own input and Embeddings. (size of each Embedding is already known by construction of the variables)\n",
    "\n",
    "`Reshape` is necessary to convert from 2D to 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embedding_dims = 64\n",
    "\n",
    "hours_input = Input(shape=(1,), name='hours_input')\n",
    "hours_embedding = Embedding(24, meta_embedding_dims)(hours_input)\n",
    "hours_reshape = Reshape((meta_embedding_dims,))(hours_embedding)\n",
    "\n",
    "dayofweeks_input = Input(shape=(1,), name='dayofweeks_input')\n",
    "dayofweeks_embedding = Embedding(7, meta_embedding_dims)(dayofweeks_input)\n",
    "dayofweeks_reshape = Reshape((meta_embedding_dims,))(dayofweeks_embedding)\n",
    "\n",
    "minutes_input = Input(shape=(1,), name='minutes_input')\n",
    "minutes_embedding = Embedding(60, meta_embedding_dims)(minutes_input)\n",
    "minutes_reshape = Reshape((meta_embedding_dims,))(minutes_embedding)\n",
    "\n",
    "dayofyears_input = Input(shape=(1,), name='dayofyears_input')\n",
    "dayofyears_embedding = Embedding(366, meta_embedding_dims)(dayofyears_input)\n",
    "dayofyears_reshape = Reshape((meta_embedding_dims,))(dayofyears_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minutes and dayofyears are single scalars; no need to `Reshape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Branches and Complete Model\n",
    "\n",
    "Combine the 5 embeddings (306D total), add a FC layer to understand latent characteristic, output 1 value for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = concatenate([titles_pooling, hours_reshape, dayofweeks_reshape, minutes_reshape, dayofyears_reshape])\n",
    "\n",
    "hidden_1 = Dense(256, activation='relu')(merged)\n",
    "hidden_1 = BatchNormalization()(hidden_1)\n",
    "\n",
    "main_output = Dense(1, activation='sigmoid', name='main_out')(hidden_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "titles_input (InputLayer)        (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hours_input (InputLayer)         (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dayofweeks_input (InputLayer)    (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "minutes_input (InputLayer)       (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dayofyears_input (InputLayer)    (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 20, 50)        2000050                                      \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 64)         1536                                         \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1, 64)         448                                          \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 1, 64)         3840                                         \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 1, 64)         23424                                        \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glob (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 306)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           78592                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 256)           1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "main_out (Dense)                 (None, 1)             257                                          \n",
      "____________________________________________________________________________________________________\n",
      "aux_out (Dense)                  (None, 1)             51                                           \n",
      "====================================================================================================\n",
      "Total params: 2,109,222.0\n",
      "Trainable params: 2,108,710.0\n",
      "Non-trainable params: 512.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[titles_input,\n",
    "                      hours_input,\n",
    "                      dayofweeks_input,\n",
    "                      minutes_input,\n",
    "                      dayofyears_input], outputs=[main_output, aux_output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss_weights=[1, 0.2])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "plot_model(model, to_file='model_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model!\n",
    "\n",
    "Randomize the model before training, since Keras [takes the last 20%](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed) as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(123)\n",
    "split = 0.2\n",
    "\n",
    "# returns randomized indices with no repeats\n",
    "idx = sample(range(titles_tf.shape[0]), titles_tf.shape[0])\n",
    "\n",
    "titles_tf = titles_tf[idx, :]\n",
    "hours = hours[idx]\n",
    "dayofweeks = dayofweeks[idx]\n",
    "minutes = minutes[idx]\n",
    "dayofyears_tf = dayofyears_tf[idx]\n",
    "is_top_submission = is_top_submission[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine No-Information Rate of the test set: the `val_main_out_acc` must be better than it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64137998126\n"
     ]
    }
   ],
   "source": [
    "print(1 - np.mean(is_top_submission[:(int(titles_tf.shape[0] * split))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model (finally!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 781230 samples, validate on 195308 samples\n",
      "Epoch 1/20\n",
      "781230/781230 [==============================] - 650s - loss: 0.6932 - main_out_loss: 0.5753 - aux_out_loss: 0.5894 - main_out_acc: 0.6590 - aux_out_acc: 0.6524 - val_loss: 0.6799 - val_main_out_loss: 0.5635 - val_aux_out_loss: 0.5819 - val_main_out_acc: 0.6682 - val_aux_out_acc: 0.6558\n",
      "Epoch 2/20\n",
      "781230/781230 [==============================] - 652s - loss: 0.6716 - main_out_loss: 0.5569 - aux_out_loss: 0.5736 - main_out_acc: 0.6737 - aux_out_acc: 0.6658 - val_loss: 0.6778 - val_main_out_loss: 0.5615 - val_aux_out_loss: 0.5816 - val_main_out_acc: 0.6698 - val_aux_out_acc: 0.6611\n",
      "Epoch 3/20\n",
      "781230/781230 [==============================] - 736s - loss: 0.6645 - main_out_loss: 0.5505 - aux_out_loss: 0.5700 - main_out_acc: 0.6799 - aux_out_acc: 0.6697 - val_loss: 0.6782 - val_main_out_loss: 0.5617 - val_aux_out_loss: 0.5822 - val_main_out_acc: 0.6692 - val_aux_out_acc: 0.6611\n",
      "Epoch 4/20\n",
      "781230/781230 [==============================] - 800s - loss: 0.6593 - main_out_loss: 0.5458 - aux_out_loss: 0.5678 - main_out_acc: 0.6849 - aux_out_acc: 0.6724 - val_loss: 0.6776 - val_main_out_loss: 0.5610 - val_aux_out_loss: 0.5828 - val_main_out_acc: 0.6695 - val_aux_out_acc: 0.6584\n",
      "Epoch 5/20\n",
      "781230/781230 [==============================] - 747s - loss: 0.6553 - main_out_loss: 0.5420 - aux_out_loss: 0.5662 - main_out_acc: 0.6871 - aux_out_acc: 0.6745 - val_loss: 0.6794 - val_main_out_loss: 0.5626 - val_aux_out_loss: 0.5841 - val_main_out_acc: 0.6705 - val_aux_out_acc: 0.6561\n",
      "Epoch 6/20\n",
      "781230/781230 [==============================] - 704s - loss: 0.6510 - main_out_loss: 0.5380 - aux_out_loss: 0.5650 - main_out_acc: 0.6914 - aux_out_acc: 0.6757 - val_loss: 0.6799 - val_main_out_loss: 0.5632 - val_aux_out_loss: 0.5836 - val_main_out_acc: 0.6690 - val_aux_out_acc: 0.6580\n",
      "Epoch 7/20\n",
      "781230/781230 [==============================] - 678s - loss: 0.6474 - main_out_loss: 0.5346 - aux_out_loss: 0.5640 - main_out_acc: 0.6944 - aux_out_acc: 0.6772 - val_loss: 0.6812 - val_main_out_loss: 0.5644 - val_aux_out_loss: 0.5842 - val_main_out_acc: 0.6690 - val_aux_out_acc: 0.6603\n",
      "Epoch 8/20\n",
      "781230/781230 [==============================] - 677s - loss: 0.6435 - main_out_loss: 0.5308 - aux_out_loss: 0.5631 - main_out_acc: 0.6976 - aux_out_acc: 0.6784 - val_loss: 0.6863 - val_main_out_loss: 0.5691 - val_aux_out_loss: 0.5860 - val_main_out_acc: 0.6675 - val_aux_out_acc: 0.6558\n",
      "Epoch 9/20\n",
      "781230/781230 [==============================] - 686s - loss: 0.6394 - main_out_loss: 0.5269 - aux_out_loss: 0.5624 - main_out_acc: 0.7007 - aux_out_acc: 0.6793 - val_loss: 0.6843 - val_main_out_loss: 0.5673 - val_aux_out_loss: 0.5852 - val_main_out_acc: 0.6675 - val_aux_out_acc: 0.6597\n",
      "Epoch 10/20\n",
      "781230/781230 [==============================] - 686s - loss: 0.6357 - main_out_loss: 0.5233 - aux_out_loss: 0.5617 - main_out_acc: 0.7044 - aux_out_acc: 0.6800 - val_loss: 0.6867 - val_main_out_loss: 0.5696 - val_aux_out_loss: 0.5854 - val_main_out_acc: 0.6675 - val_aux_out_acc: 0.6583\n",
      "Epoch 11/20\n",
      "781230/781230 [==============================] - 692s - loss: 0.6323 - main_out_loss: 0.5201 - aux_out_loss: 0.5612 - main_out_acc: 0.7069 - aux_out_acc: 0.6807 - val_loss: 0.6900 - val_main_out_loss: 0.5728 - val_aux_out_loss: 0.5859 - val_main_out_acc: 0.6671 - val_aux_out_acc: 0.6603\n",
      "Epoch 12/20\n",
      "781230/781230 [==============================] - 684s - loss: 0.6286 - main_out_loss: 0.5165 - aux_out_loss: 0.5607 - main_out_acc: 0.7100 - aux_out_acc: 0.6813 - val_loss: 0.6913 - val_main_out_loss: 0.5740 - val_aux_out_loss: 0.5861 - val_main_out_acc: 0.6644 - val_aux_out_acc: 0.6598\n",
      "Epoch 13/20\n",
      "781230/781230 [==============================] - 688s - loss: 0.6249 - main_out_loss: 0.5129 - aux_out_loss: 0.5602 - main_out_acc: 0.7132 - aux_out_acc: 0.6818 - val_loss: 0.6933 - val_main_out_loss: 0.5759 - val_aux_out_loss: 0.5868 - val_main_out_acc: 0.6651 - val_aux_out_acc: 0.6590\n",
      "Epoch 14/20\n",
      "781230/781230 [==============================] - 682s - loss: 0.6213 - main_out_loss: 0.5093 - aux_out_loss: 0.5599 - main_out_acc: 0.7169 - aux_out_acc: 0.6820 - val_loss: 0.6988 - val_main_out_loss: 0.5815 - val_aux_out_loss: 0.5865 - val_main_out_acc: 0.6613 - val_aux_out_acc: 0.6581\n",
      "Epoch 15/20\n",
      "781230/781230 [==============================] - 685s - loss: 0.6179 - main_out_loss: 0.5060 - aux_out_loss: 0.5596 - main_out_acc: 0.7197 - aux_out_acc: 0.6829 - val_loss: 0.7001 - val_main_out_loss: 0.5828 - val_aux_out_loss: 0.5866 - val_main_out_acc: 0.6625 - val_aux_out_acc: 0.6579\n",
      "Epoch 16/20\n",
      "781230/781230 [==============================] - 686s - loss: 0.6139 - main_out_loss: 0.5021 - aux_out_loss: 0.5593 - main_out_acc: 0.7227 - aux_out_acc: 0.6831 - val_loss: 0.7033 - val_main_out_loss: 0.5858 - val_aux_out_loss: 0.5875 - val_main_out_acc: 0.6600 - val_aux_out_acc: 0.6585\n",
      "Epoch 17/20\n",
      "781230/781230 [==============================] - 669s - loss: 0.6107 - main_out_loss: 0.4989 - aux_out_loss: 0.5590 - main_out_acc: 0.7255 - aux_out_acc: 0.6832 - val_loss: 0.7076 - val_main_out_loss: 0.5901 - val_aux_out_loss: 0.5875 - val_main_out_acc: 0.6612 - val_aux_out_acc: 0.6589\n",
      "Epoch 18/20\n",
      "781230/781230 [==============================] - 712s - loss: 0.6075 - main_out_loss: 0.4957 - aux_out_loss: 0.5588 - main_out_acc: 0.7284 - aux_out_acc: 0.6833 - val_loss: 0.7094 - val_main_out_loss: 0.5918 - val_aux_out_loss: 0.5881 - val_main_out_acc: 0.6596 - val_aux_out_acc: 0.6591\n",
      "Epoch 19/20\n",
      "781230/781230 [==============================] - 688s - loss: 0.6038 - main_out_loss: 0.4921 - aux_out_loss: 0.5585 - main_out_acc: 0.7314 - aux_out_acc: 0.6837 - val_loss: 0.7149 - val_main_out_loss: 0.5972 - val_aux_out_loss: 0.5884 - val_main_out_acc: 0.6574 - val_aux_out_acc: 0.6588\n",
      "Epoch 20/20\n",
      "781230/781230 [==============================] - 690s - loss: 0.6012 - main_out_loss: 0.4895 - aux_out_loss: 0.5583 - main_out_acc: 0.7333 - aux_out_acc: 0.6836 - val_loss: 0.7196 - val_main_out_loss: 0.6018 - val_aux_out_loss: 0.5889 - val_main_out_acc: 0.6594 - val_aux_out_acc: 0.6601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a7be898>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([titles_tf, hours, dayofweeks, minutes, dayofyears_tf], [is_top_submission, is_top_submission],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=split, callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model to Make Predictions\n",
    "\n",
    "Use the created model to optimize a submission to /r/AskReddit.\n",
    "\n",
    "Example submission with only 4 upvotes: [Which movie's plot would drastically change if you removed a letter from its title?](https://www.reddit.com/r/AskReddit/comments/5odcpd/which_movies_plot_would_drastically_change_if_you/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Text (fixing Time Submitted)\n",
    "\n",
    "First create a function to include arbitrary text as model-acceptable format. (using dictionary created from the Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(text, maxlen):\n",
    "    encoded = word_tokenizer.texts_to_sequences([text])\n",
    "    return sequence.pad_sequences(encoded, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0   68 6380  923   18 3403  142   23    1\n",
      "  1513    5 1146   44  302  508]]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would drastically change if you removed a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a baseline using the real values.\n",
    "\n",
    "The first value (index 0) is the value from the `main_output`, the second is from the fasttext `aux_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.66917652]], dtype=float32), array([[ 0.40641001]], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_hour = np.array([15])\n",
    "input_minute = np.array([46])\n",
    "input_dayofweek = np.array([1])\n",
    "input_dayofyear = np.array([16 - 1])\n",
    "\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So can see if we can to perform better than 0.67 probability. Make minor changes without avertly changing meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.68286121]], dtype=float32), array([[ 0.40531182]], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would change if you removed a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing \"drastically\" makes things better.\n",
    "\n",
    "How about fixing the tense of \"removed\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.69150311]], dtype=float32), array([[ 0.4588666]], dtype=float32)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would change if you remove a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability increased. From that headline, how about changing \"Which\" to \"What?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.7321561]], dtype=float32), array([[ 0.51213974]], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What movie's plot would change if you remove a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using more emphasis: add \"single\" to letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.75322324]], dtype=float32), array([[ 0.55223811]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What movie's plot would change if you remove a single letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Time Submitted (fixing Text)\n",
    "\n",
    "Do a hyperparameter grid search for all combinations of `hour`, `minute`, and `dayofweek` (`dayofyear` must be offset accordingly with `dayofweek`) and record the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_array = []\n",
    "\n",
    "for input_dayofweek in range(6):\n",
    "    for input_hour in range(23):\n",
    "        for input_minute in range(59):\n",
    "            results_array.append([input_dayofweek,\n",
    "                                  input_hour,\n",
    "                                  input_minute,\n",
    "                                  model.predict([encoded_text,\n",
    "                                                 np.array([input_hour]),\n",
    "                                                 np.array([input_dayofweek]),\n",
    "                                                 np.array([input_minute]),\n",
    "                                                 input_dayofyear + input_dayofweek - 1])[0]])\n",
    "            \n",
    "results_array = np.array(results_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.          22.          55.           0.84148395]\n",
      " [  0.          22.          13.           0.83928609]\n",
      " [  0.          22.           0.           0.83868033]\n",
      " [  0.          22.          49.           0.83778793]\n",
      " [  0.          22.          27.           0.83483583]\n",
      " [  0.          22.          19.           0.83461344]\n",
      " [  0.          22.          16.           0.83399975]\n",
      " [  0.          22.          39.           0.83264273]\n",
      " [  0.          22.          47.           0.82798171]\n",
      " [  0.          22.          33.           0.82755792]\n",
      " [  0.          22.          43.           0.82691711]\n",
      " [  0.          22.          42.           0.82675755]\n",
      " [  0.          22.          58.           0.82663274]\n",
      " [  0.          22.          10.           0.82661295]\n",
      " [  0.          22.          37.           0.82582957]\n",
      " [  0.          22.          25.           0.82406569]\n",
      " [  0.          22.          12.           0.82374126]\n",
      " [  0.          21.          13.           0.82372391]\n",
      " [  2.           4.           5.           0.82362038]\n",
      " [  0.          22.           8.           0.82283509]]\n"
     ]
    }
   ],
   "source": [
    "print(results_array[results_array[:,3].argsort()[::-1]][0:20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract Embeddings\n",
    "\n",
    "Add the embeddings for each Embeddings layer into a .txt file. (skipping word vectors, since the resulting data file will be huge.\n",
    "\n",
    "Create a helper function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embeddings(weights, file_name):\n",
    "    f = open(file_name + \".txt\", 'w')\n",
    "    for i in range(weights.shape[0]):\n",
    "            f.write(str(i) + \" \" + \" \".join(str(x) for x in weights[i]) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers are in the same order as they were instantiated (can look at summary to get index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_embeddings(model.layers[6].get_weights()[0], \"hours_embeddings\")\n",
    "write_embeddings(model.layers[7].get_weights()[0], \"dayofweeks_embeddings\")\n",
    "write_embeddings(model.layers[8].get_weights()[0], \"minutes_embeddings\")\n",
    "\n",
    "# remember to add +1 back to index when doing analysis\n",
    "# additionally, the last value corresponds to i = 366\n",
    "write_embeddings(model.layers[9].get_weights()[0], \"dayofyears_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LICENSE\n",
    "\n",
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2017 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
