{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Success of a Reddit Submission with Deep Learning and Keras\n",
    "\n",
    "2016-06-26\n",
    "\n",
    "by Max Woolf ([@minimaxir](http://minimaxir.com))\n",
    "\n",
    "*Max's open-source projects are supported by his [Patreon](https://www.patreon.com/minimaxir). If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "BigQuery used to get data:\n",
    "\n",
    "```sql\n",
    "#standardSQL \n",
    "SELECT id, title,\n",
    "  CAST(FORMAT_TIMESTAMP('%H', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS hour,\n",
    "  CAST(FORMAT_TIMESTAMP('%M', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS minute,\n",
    "  CAST(FORMAT_TIMESTAMP('%w', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS dayofweek,\n",
    "  CAST(FORMAT_TIMESTAMP('%j', TIMESTAMP_SECONDS(created_utc), 'America/New_York') AS INT64) AS dayofyear,\n",
    "  IF(PERCENT_RANK() OVER (ORDER BY score ASC) >= 0.50, 1, 0) as is_top_submission\n",
    "  FROM `fh-bigquery.reddit_posts.*`\n",
    "  WHERE (_TABLE_SUFFIX BETWEEN '2017_01' AND '2017_04')\n",
    "  AND subreddit = 'AskReddit'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from random import random, sample, seed\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "data_path = '../ErdosInstitute-SIG_Project/Data/wsb_full.csv'\n",
    "embeddings_path = '../ErdosInstitute-SIG_Project/Data/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now, we follow Woolf\n",
    "#let's keep the removed/deleted posts and only look at titles\n",
    "dfog = pd.read_csv(data_path, low_memory=False)\n",
    "dfog=dfog.loc[((dfog.is_self==True) & ~(dfog[\"title\"].str.contains(\"Thread|thread|Sunday Live Chat|consolidation zone|Containment Zone|Daily Discussion|Daily discussion|Saturday Chat|What Are Your Moves Tomorrow|What Are Your Moves Today|MEGATHREAD\",na=False)))]\n",
    "df=dfog.dropna(subset = ['title'])\n",
    "#dfog=dfog.loc[(((dfog.removed_by_category.isnull()))) & ((dfog.is_self==True) & ~(dfog[\"title\"].str.contains(\"Thread|thread|Sunday Live Chat|consolidation zone|Containment Zone|Daily Discussion|Daily discussion|Saturday Chat|What Are Your Moves Tomorrow|What Are Your Moves Today|MEGATHREAD\",na=False)))]\n",
    "#df=dfog.dropna(subset = ['title', 'selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-75e8bc7c8810>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['utc']=df.created_utc.apply(lambda x : datetime.utcfromtimestamp(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         2020-01-01 23:53:34\n",
       "1         2020-01-01 23:53:28\n",
       "2         2020-01-01 23:50:05\n",
       "4         2020-01-01 23:30:47\n",
       "5         2020-01-01 23:24:34\n",
       "                  ...        \n",
       "1260225   2021-05-06 00:06:57\n",
       "1260228   2021-05-06 00:05:20\n",
       "1260229   2021-05-06 00:05:16\n",
       "1260232   2021-05-06 00:02:57\n",
       "1260234   2021-05-06 00:02:43\n",
       "Name: utc, Length: 723982, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['utc']=df.created_utc.apply(lambda x : datetime.utcfromtimestamp(x))\n",
    "df['utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoodPost(ups,threshold=1):\n",
    "    if ups>threshold:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles = []\n",
    "#hours = []\n",
    "#minutes = []\n",
    "#dayofweeks = []\n",
    "#dayofyears = []\n",
    "#is_top_submission = []\n",
    "\n",
    "#with open(data_path, 'r', encoding=\"latin1\") as f:\n",
    "#    reader = csv.DictReader(f)\n",
    "#    for submission in reader:\n",
    "#        titles.append(submission['title'])\n",
    "#        hours.append(submission['hour'])\n",
    "#        minutes.append(submission['minute'])\n",
    "#        dayofweeks.append(submission['dayofweek'])\n",
    "#        dayofyears.append(submission['dayofyear'])\n",
    "#        is_top_submission.append(submission['is_top_submission'])\n",
    "            \n",
    "titles = np.array(df.title)\n",
    "hours = np.array(df.utc.apply(lambda x : x.hour), dtype=int)\n",
    "minutes = np.array(df.utc.apply(lambda x : x.minute), dtype=int)\n",
    "dayofweeks = np.array(df.utc.apply(lambda x : x.weekday()), dtype=int)\n",
    "dayofyears = np.array(df.utc.apply(lambda x : x.timetuple().tm_yday), dtype=int)\n",
    "is_top_submission = np.array(df.ups.apply(lambda x : GoodPost(x)), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good time to get on $BLUE'\n",
      " \"i wanna buy a call but i don't want to bet too much\"]\n",
      "(723982,)\n",
      "[23 23]\n",
      "[53 53]\n",
      "[2 2]\n",
      "[1 1]\n",
      "[1 0]\n",
      "But FAMI\n"
     ]
    }
   ],
   "source": [
    "print(titles[0:2])\n",
    "print(titles.shape)\n",
    "print(hours[0:2])\n",
    "print(minutes[0:2])\n",
    "print(dayofweeks[0:2])\n",
    "print(dayofyears[0:2])\n",
    "print(is_top_submission[0:2])\n",
    "print(titles[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  Good time to get on $BLUE\n",
       "1          i wanna buy a call but i don't want to bet too...\n",
       "2                                              Buy INTU - DD\n",
       "4                                            I will remember\n",
       "5          Change my mind: Nothing is priced in, Stock ma...\n",
       "                                 ...                        \n",
       "1260225                                   Look into buy FAMI\n",
       "1260228                                             But FAMI\n",
       "1260229                                                 COMS\n",
       "1260232                        ASO technical breakout and DD\n",
       "1260234                                     Rkt to the moon!\n",
       "Name: title, Length: 723982, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7856949482169446"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.mean(is_top_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The No-Information Rate is 64% (i.e. simply say all AskReddit submissions are terrible), so any model trained must do better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process /r/AskReddit Submission Title Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('good', 9208), ('time', 13379), ('to', 178860), ('get', 20202), ('on', 73332), ('blue'\n",
      "{'the': 1, 'to': 2, 'is': 3, 'gme': 4, 'and': 5, 'a': 6, 'i': 7, 'on': 8, 'for': 9, 'of': 10, 'in': \n",
      "111305\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "max_features = 40000\n",
    "\n",
    "word_tokenizer = Tokenizer(max_features)\n",
    "word_tokenizer.fit_on_texts(titles)\n",
    "\n",
    "print(str(word_tokenizer.word_counts)[0:100])\n",
    "print(str(word_tokenizer.word_index)[0:100])\n",
    "print(len(word_tokenizer.word_counts))   # true word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 67, 2, 43, 8, 1626]\n"
     ]
    }
   ],
   "source": [
    "titles_tf = word_tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "print(titles_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   97   67    2   43    8 1626]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "titles_tf = sequence.pad_sequences(titles_tf, maxlen=maxlen)\n",
    "\n",
    "print(titles_tf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Pretrained Embeddings\n",
    "\n",
    "Adapted from [the official keras tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "\n",
    "Use pretrained GloVe embeddings to both give Embeddings training a good start, and to account for words that might be present in the test set but not in the training set.\n",
    "\n",
    "First, load the 50D embeddings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.8427e-01  4.7977e-02 -1.5062e-01  1.9418e-02 -1.0963e-01  1.8222e-01\n",
      " -1.1821e-01 -2.0882e-01  1.0880e-01 -2.0259e+00  2.7138e-01 -1.4365e-01\n",
      "  8.4653e-02 -8.8175e-02 -1.6669e-01  1.2589e-01 -3.0433e-01 -2.5740e-01\n",
      "  2.4622e-01  3.1348e-01  2.9160e-01  6.5911e-01  3.4504e-01 -3.5690e-03\n",
      " -3.4488e-01 -1.4918e-01  6.4012e-02 -1.7979e-01  9.0562e-02 -2.7049e-01\n",
      " -1.7458e-01  3.8784e-01 -4.3893e-01 -6.7898e-01 -1.5777e+00  4.7780e-01\n",
      " -4.0246e-01 -4.1838e-02 -1.5407e-01 -4.5389e-02  1.1857e-01 -3.3515e-01\n",
      " -2.0487e-01 -3.5495e-02  1.5259e-01  1.8501e-02  8.0593e-01  2.9267e-01\n",
      " -1.7255e-01  8.1989e-02  2.6326e-01 -2.5286e-01 -7.5795e-02 -1.7749e-01\n",
      " -5.6662e-01  5.0391e-01  1.7611e-01  5.2120e-02  1.4223e-01  1.1035e-01\n",
      "  7.4027e-01 -9.6867e-02  8.3969e-02  7.9725e-01  3.7040e-03 -2.7290e-01\n",
      "  8.7329e-02 -5.2325e-02  1.8124e-01  7.5028e-02  4.7539e-02 -3.0228e-01\n",
      " -1.0071e-01  5.9970e-01  3.7499e-02  9.6091e-03 -1.1767e-01  2.1654e-01\n",
      " -1.0530e-01 -7.4920e-01 -2.6626e-01  8.9381e-02  5.7775e-01 -3.4076e-01\n",
      " -1.1215e-01  1.3016e-01 -5.4676e-01  6.4580e-01 -2.1730e-01  8.7133e-02\n",
      " -5.6180e-01  5.2618e-01 -3.2394e-01 -7.5991e-01  5.2866e-02  2.5932e-01\n",
      " -4.2680e-01 -9.3674e-02  2.1914e-01 -4.0569e-01 -1.5567e-01  1.2445e-02\n",
      " -2.2551e-01 -1.2596e-01 -1.9577e-01 -2.7043e-01  3.3908e-01  4.5641e-02\n",
      " -1.4513e-01  2.0240e-01  3.5655e-02 -4.2273e-01 -1.4876e-01 -6.1991e-01\n",
      "  2.2532e-01  3.4393e-01 -1.9300e-01  1.9224e-01 -5.0948e-02 -1.8799e-01\n",
      "  3.4954e-02 -2.1933e-01  1.6072e-01  4.5311e-01  2.1567e-01 -4.8854e-01\n",
      "  3.4638e-01  3.2015e-01  7.9632e-02  1.3925e-01  5.3335e-01 -1.2667e-01\n",
      "  1.0702e-01  1.7311e-01 -1.5202e-01  1.8213e-02 -2.8562e-01  1.0621e-01\n",
      " -1.1496e-01  5.8368e-01 -7.3371e-02  2.1681e-01  9.3336e-02  3.1245e-01\n",
      " -7.7723e-01 -2.5419e-01 -4.0186e-01 -1.5958e-02 -1.0297e-01  2.2091e-01\n",
      " -6.8557e-01 -1.9566e-02  1.6302e-01 -3.4652e-01  2.8779e-01  5.7854e-02\n",
      "  2.8703e-01 -2.2756e-01  2.6920e-01 -1.8895e-01 -1.1502e-02 -2.1505e-01\n",
      " -1.9931e-02 -2.3387e-01 -9.0040e-02  3.0702e-01  8.6308e-03  3.3419e-01\n",
      " -7.0714e-02 -4.6313e-02 -3.4970e-01  2.5307e-01 -8.2135e-01  5.2109e-03\n",
      "  1.8186e-01 -2.5161e-01 -1.1474e-01  4.1138e-01  2.2250e-01 -9.8922e-02\n",
      "  1.4910e-01 -4.2380e-01  3.2967e-01 -3.3345e-02 -1.5568e-01 -1.2330e-01\n",
      "  5.1824e-01  2.8310e-01  1.8567e-01  2.2334e-01 -6.1495e-02  3.4828e-01\n",
      "  1.9721e-01 -2.6426e-02  9.6368e-03 -3.0547e-01  1.0748e-01  1.4175e-01\n",
      "  3.1804e-01 -3.7154e-01  1.7515e+00 -2.4757e-03  3.3379e-01  1.7365e-01\n",
      "  1.8051e-01  1.3384e-01  1.1731e-01  2.6252e-01 -8.5366e-02 -4.8810e-01\n",
      " -5.8555e-01 -4.3722e-01  2.3302e-01 -1.1417e-01 -2.7498e-01  1.2396e-01\n",
      "  1.8479e-01  3.7328e-01 -8.9720e-02  5.3090e-02 -8.9491e-02  3.7095e-01\n",
      " -2.6497e-01  2.3513e-01  1.4425e-01 -3.2658e-01  3.5652e-01  3.2405e-01\n",
      "  1.0638e-01 -1.4149e-01  5.5244e-02  1.1346e-01  1.2900e-01 -7.0625e-01\n",
      "  3.2189e-03  1.3113e-01 -2.3099e-01 -1.2951e-01 -2.7444e-01 -2.2296e-01\n",
      " -9.5771e-02 -3.1084e-02  3.2240e-01 -1.6385e-02 -3.7818e-01 -1.6563e-01\n",
      " -2.2267e-01  1.1651e-01 -1.4664e-01 -3.5786e-01  2.6073e-01 -2.7318e-01\n",
      " -3.0501e-01  1.6742e-01  4.7574e-01  2.3245e-01  9.3751e-02 -2.6701e-01\n",
      "  1.0637e-02 -3.9020e-01  8.4491e-02 -5.9185e-01  7.4939e-02 -3.6574e-02\n",
      "  1.0965e-01 -9.9811e-02 -1.5383e-01  4.1754e-03  1.1249e-02  3.2394e-01\n",
      " -4.1205e-02 -2.6279e-01  3.0848e-01  1.4001e-01 -1.4057e-01  6.7458e-02\n",
      " -2.5609e+00  9.4188e-02  4.2045e-02 -4.7340e-01 -2.6729e-01 -5.2446e-02\n",
      " -9.5423e-03 -3.8169e-02 -2.2409e-01  4.0835e-01  2.5794e-01 -2.6793e-01\n",
      " -4.0305e-01 -3.9295e-01  1.6047e-01 -3.8285e-01 -2.1442e-01 -1.4769e-01\n",
      "  1.8154e-01 -8.2520e-01 -2.8249e-03 -9.0071e-02  1.6922e-02  2.9278e-01]\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = {}\n",
    "\n",
    "with open(embeddings_path, 'r',encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = np.array(line_split[1:], dtype=float)\n",
    "        word = line_split[0]\n",
    "        embedding_vectors[word] = vec\n",
    "        \n",
    "print(embedding_vectors['you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights matrix as zeroes, then replace the corresponding index of the weights matrix with the index of the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00]\n",
      " [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      "  -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "   5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      "  -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      "  -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "   9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "   2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      "  -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      "  -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "   7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      "  -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "   1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "   1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      "  -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "   7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      "  -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      "  -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "   2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "   1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "   8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "   1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "   7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "   1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      "  -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      "  -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "   6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      "  -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "   1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "   1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      "  -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "   2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      "  -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      "  -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      "  -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "   5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "   6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "   1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      "  -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      "  -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "   3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      "  -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "   1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "   1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "   2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      "  -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      "  -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      "  -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      "  -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "   6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "   3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]]\n"
     ]
    }
   ],
   "source": [
    "weights_matrix = np.zeros((max_features + 1, 300))\n",
    "\n",
    "for word, i in word_tokenizer.word_index.items():\n",
    "\n",
    "    embedding_vector = embedding_vectors.get(word)\n",
    "    if embedding_vector is not None and i <= max_features:\n",
    "        weights_matrix[i] = embedding_vector\n",
    "\n",
    "# index 0 vector should be all zeroes, index 1 vector should be the same one as above\n",
    "print(weights_matrix[0:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Other Metadata\n",
    "\n",
    "All metadata must be zero-indexed integers.\n",
    "\n",
    "* `hours` in the correct format. (`0` = 12AM EST, `23` = 11PM EST)\n",
    "* `dayofweeks` is in the correct format (`0` = Sunday, `6` = Saturday) (for me 0 is Monday)\n",
    "* `minutes` is in the correct format verbatim.\n",
    "* `dayofyears` is 1-indexed, so must subtract 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dayofyears_tf = dayofyears - 1\n",
    "\n",
    "max(dayofyears_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "Use Keras's functional API to build a branching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation\n",
    "from keras.layers.core import Masking, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Branch\n",
    "\n",
    "Encode the text using a mock fasttext approach. Use `weights_matrix` derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_input = Input(shape=(maxlen,), name='titles_input')\n",
    "titles_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(titles_input)\n",
    "titles_pooling = GlobalAveragePooling1D()(titles_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an auxillary output to regularize the text component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_output = Dense(1, activation='sigmoid', name='aux_out')(titles_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Branch\n",
    "\n",
    "Each metadata variable gets its own input and Embeddings. (size of each Embedding is already known by construction of the variables)\n",
    "\n",
    "`Reshape` is necessary to convert from 2D to 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embedding_dims = 64\n",
    "\n",
    "hours_input = Input(shape=(1,), name='hours_input')\n",
    "hours_embedding = Embedding(24, meta_embedding_dims)(hours_input)\n",
    "hours_reshape = Reshape((meta_embedding_dims,))(hours_embedding)\n",
    "\n",
    "dayofweeks_input = Input(shape=(1,), name='dayofweeks_input')\n",
    "dayofweeks_embedding = Embedding(7, meta_embedding_dims)(dayofweeks_input)\n",
    "dayofweeks_reshape = Reshape((meta_embedding_dims,))(dayofweeks_embedding)\n",
    "\n",
    "minutes_input = Input(shape=(1,), name='minutes_input')\n",
    "minutes_embedding = Embedding(60, meta_embedding_dims)(minutes_input)\n",
    "minutes_reshape = Reshape((meta_embedding_dims,))(minutes_embedding)\n",
    "\n",
    "dayofyears_input = Input(shape=(1,), name='dayofyears_input')\n",
    "dayofyears_embedding = Embedding(366, meta_embedding_dims)(dayofyears_input)\n",
    "dayofyears_reshape = Reshape((meta_embedding_dims,))(dayofyears_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minutes and dayofyears are single scalars; no need to `Reshape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Branches and Complete Model\n",
    "\n",
    "Combine the 5 embeddings (306D total), add a FC layer to understand latent characteristic, output 1 value for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = concatenate([titles_pooling, hours_reshape, dayofweeks_reshape, minutes_reshape, dayofyears_reshape])\n",
    "\n",
    "hidden_1 = Dense(256, activation='relu')(merged)\n",
    "hidden_1 = BatchNormalization()(hidden_1)\n",
    "\n",
    "main_output = Dense(1, activation='sigmoid', name='main_out')(hidden_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "titles_input (InputLayer)       [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hours_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dayofweeks_input (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "minutes_input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dayofyears_input (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 300)      12000300    titles_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 64)        1536        hours_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 64)        448         dayofweeks_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 64)        3840        minutes_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 64)        23424       dayofyears_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 300)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 64)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 64)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 64)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 556)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          142592      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "main_out (Dense)                (None, 1)            257         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "aux_out (Dense)                 (None, 1)            301         global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 12,173,722\n",
      "Trainable params: 12,173,210\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[titles_input,\n",
    "                      hours_input,\n",
    "                      dayofweeks_input,\n",
    "                      minutes_input,\n",
    "                      dayofyears_input], outputs=[main_output, aux_output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss_weights=[1, 0.2])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#####CURRENTLY DOESN'T WORK, BUT JUST PICTURES\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "plot_model(model, to_file='model_shapes.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model!\n",
    "\n",
    "Randomize the model before training, since Keras [takes the last 20%](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed) as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(123)\n",
    "split = 0.2\n",
    "\n",
    "# returns randomized indices with no repeats\n",
    "idx = sample(range(titles_tf.shape[0]), titles_tf.shape[0])\n",
    "\n",
    "titles_tf = titles_tf[idx, :]\n",
    "hours = hours[idx]\n",
    "dayofweeks = dayofweeks[idx]\n",
    "minutes = minutes[idx]\n",
    "dayofyears_tf = dayofyears_tf[idx]\n",
    "is_top_submission = is_top_submission[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine No-Information Rate of the test set: the `val_main_out_acc` must be better than it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785470593110307\n"
     ]
    }
   ],
   "source": [
    "print(1 - np.mean(is_top_submission[:(int(titles_tf.shape[0] * split))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model (finally!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18100/18100 [==============================] - 3496s 192ms/step - loss: 0.5239 - main_out_loss: 0.4228 - aux_out_loss: 0.5058 - main_out_accuracy: 0.8011 - aux_out_accuracy: 0.7857 - val_loss: 0.4886 - val_main_out_loss: 0.3911 - val_aux_out_loss: 0.4875 - val_main_out_accuracy: 0.8193 - val_aux_out_accuracy: 0.7851\n",
      "Epoch 2/20\n",
      "18100/18100 [==============================] - 3503s 194ms/step - loss: 0.4794 - main_out_loss: 0.3837 - aux_out_loss: 0.4786 - main_out_accuracy: 0.8234 - aux_out_accuracy: 0.7872 - val_loss: 0.4845 - val_main_out_loss: 0.3871 - val_aux_out_loss: 0.4869 - val_main_out_accuracy: 0.8205 - val_aux_out_accuracy: 0.7854 loss: 0.4794 - main_out_loss: 0.3837 - aux_out_loss: 0.4785 - main_out_accuracy: 0.8234 - a - ETA: 5:09 - loss: 0.4794 - main_out_loss: 0.3837 - aux_out_loss: 0.4 - ETA: 2:53 - loss: 0.47 - ETA: 32s - loss: 0.4794 - main_out_loss: 0.3837 - aux_out_loss: 0.4785 - ma - ETA: 27s - loss: 0.4794 - ETA: 17s - loss: 0.4794 - main_out_loss: 0.3837 - aux_out_loss: 0.4786 - main_out_accuracy: 0.8234 - aux_out_accura - ETA: \n",
      "Epoch 3/20\n",
      "18100/18100 [==============================] - 3768s 208ms/step - loss: 0.4632 - main_out_loss: 0.3687 - aux_out_loss: 0.4726 - main_out_accuracy: 0.8327 - aux_out_accuracy: 0.7894 - val_loss: 0.4872 - val_main_out_loss: 0.3895 - val_aux_out_loss: 0.4885 - val_main_out_accuracy: 0.8215 - val_aux_out_accuracy: 0.7841\n",
      "Epoch 4/20\n",
      "18100/18100 [==============================] - 3828s 212ms/step - loss: 0.4500 - main_out_loss: 0.3560 - aux_out_loss: 0.4700 - main_out_accuracy: 0.8400 - aux_out_accuracy: 0.7900 - val_loss: 0.4943 - val_main_out_loss: 0.3959 - val_aux_out_loss: 0.4921 - val_main_out_accuracy: 0.8179 - val_aux_out_accuracy: 0.7855out_loss: 0.4\n",
      "Epoch 5/20\n",
      "18100/18100 [==============================] - 3662s 202ms/step - loss: 0.4372 - main_out_loss: 0.3436 - aux_out_loss: 0.4678 - main_out_accuracy: 0.8474 - aux_out_accuracy: 0.7907 - val_loss: 0.5064 - val_main_out_loss: 0.4084 - val_aux_out_loss: 0.4901 - val_main_out_accuracy: 0.8098 - val_aux_out_accuracy: 0.7843\n",
      "Epoch 6/20\n",
      "  192/18100 [..............................] - ETA: 59:54 - loss: 0.4326 - main_out_loss: 0.3397 - aux_out_loss: 0.4642 - main_out_accuracy: 0.8443 - aux_out_accuracy: 0.7897"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e20feea3023b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit([titles_tf, hours, dayofweeks, minutes, dayofyears_tf], [is_top_submission, is_top_submission],\n\u001b[0m\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           validation_split=split, callbacks=[csv_logger])\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([titles_tf, hours, dayofweeks, minutes, dayofyears_tf], [is_top_submission, is_top_submission],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=split, callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model to Make Predictions\n",
    "\n",
    "Use the created model to optimize a submission to /r/AskReddit.\n",
    "\n",
    "Example submission with only 4 upvotes: [Which movie's plot would drastically change if you removed a letter from its title?](https://www.reddit.com/r/AskReddit/comments/5odcpd/which_movies_plot_would_drastically_change_if_you/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Text (fixing Time Submitted)\n",
    "\n",
    "First create a function to include arbitrary text as model-acceptable format. (using dictionary created from the Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_text(text, maxlen):\n",
    "    encoded = word_tokenizer.texts_to_sequences([text])\n",
    "    return sequence.pad_sequences(encoded, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0   68 6380  923   18 3403  142   23    1\n",
      "  1513    5 1146   44  302  508]]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would drastically change if you removed a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify a baseline using the real values.\n",
    "\n",
    "The first value (index 0) is the value from the `main_output`, the second is from the fasttext `aux_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.66917652]], dtype=float32), array([[ 0.40641001]], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_hour = np.array([15])\n",
    "input_minute = np.array([46])\n",
    "input_dayofweek = np.array([1])\n",
    "input_dayofyear = np.array([16 - 1])\n",
    "\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So can see if we can to perform better than 0.67 probability. Make minor changes without avertly changing meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.68286121]], dtype=float32), array([[ 0.40531182]], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would change if you removed a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing \"drastically\" makes things better.\n",
    "\n",
    "How about fixing the tense of \"removed\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.69150311]], dtype=float32), array([[ 0.4588666]], dtype=float32)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Which movie's plot would change if you remove a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability increased. From that headline, how about changing \"Which\" to \"What?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.7321561]], dtype=float32), array([[ 0.51213974]], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What movie's plot would change if you remove a letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using more emphasis: add \"single\" to letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.75322324]], dtype=float32), array([[ 0.55223811]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What movie's plot would change if you remove a single letter from its title?\"\n",
    "encoded_text = encode_text(input_text, maxlen)\n",
    "model.predict([encoded_text, input_hour, input_dayofweek, input_minute, input_dayofyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Time Submitted (fixing Text)\n",
    "\n",
    "Do a hyperparameter grid search for all combinations of `hour`, `minute`, and `dayofweek` (`dayofyear` must be offset accordingly with `dayofweek`) and record the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_array = []\n",
    "\n",
    "for input_dayofweek in range(6):\n",
    "    for input_hour in range(23):\n",
    "        for input_minute in range(59):\n",
    "            results_array.append([input_dayofweek,\n",
    "                                  input_hour,\n",
    "                                  input_minute,\n",
    "                                  model.predict([encoded_text,\n",
    "                                                 np.array([input_hour]),\n",
    "                                                 np.array([input_dayofweek]),\n",
    "                                                 np.array([input_minute]),\n",
    "                                                 input_dayofyear + input_dayofweek - 1])[0]])\n",
    "            \n",
    "results_array = np.array(results_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.          22.          55.           0.84148395]\n",
      " [  0.          22.          13.           0.83928609]\n",
      " [  0.          22.           0.           0.83868033]\n",
      " [  0.          22.          49.           0.83778793]\n",
      " [  0.          22.          27.           0.83483583]\n",
      " [  0.          22.          19.           0.83461344]\n",
      " [  0.          22.          16.           0.83399975]\n",
      " [  0.          22.          39.           0.83264273]\n",
      " [  0.          22.          47.           0.82798171]\n",
      " [  0.          22.          33.           0.82755792]\n",
      " [  0.          22.          43.           0.82691711]\n",
      " [  0.          22.          42.           0.82675755]\n",
      " [  0.          22.          58.           0.82663274]\n",
      " [  0.          22.          10.           0.82661295]\n",
      " [  0.          22.          37.           0.82582957]\n",
      " [  0.          22.          25.           0.82406569]\n",
      " [  0.          22.          12.           0.82374126]\n",
      " [  0.          21.          13.           0.82372391]\n",
      " [  2.           4.           5.           0.82362038]\n",
      " [  0.          22.           8.           0.82283509]]\n"
     ]
    }
   ],
   "source": [
    "print(results_array[results_array[:,3].argsort()[::-1]][0:20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract Embeddings\n",
    "\n",
    "Add the embeddings for each Embeddings layer into a .txt file. (skipping word vectors, since the resulting data file will be huge.\n",
    "\n",
    "Create a helper function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embeddings(weights, file_name):\n",
    "    f = open(file_name + \".txt\", 'w')\n",
    "    for i in range(weights.shape[0]):\n",
    "            f.write(str(i) + \" \" + \" \".join(str(x) for x in weights[i]) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers are in the same order as they were instantiated (can look at summary to get index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_embeddings(model.layers[6].get_weights()[0], \"hours_embeddings\")\n",
    "write_embeddings(model.layers[7].get_weights()[0], \"dayofweeks_embeddings\")\n",
    "write_embeddings(model.layers[8].get_weights()[0], \"minutes_embeddings\")\n",
    "\n",
    "# remember to add +1 back to index when doing analysis\n",
    "# additionally, the last value corresponds to i = 366\n",
    "write_embeddings(model.layers[9].get_weights()[0], \"dayofyears_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LICENSE\n",
    "\n",
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2017 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
