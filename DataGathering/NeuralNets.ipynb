{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e88e24a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 25], [1, 44], [29, 18], [23, 44], [45], [44], [22, 18], [12, 1], [22, 44], [23, 48, 25, 9]]\n",
      "tf.Tensor(\n",
      "[[33 25  0  0]\n",
      " [ 1 44  0  0]\n",
      " [29 18  0  0]\n",
      " [23 44  0  0]\n",
      " [45  0  0  0]\n",
      " [44  0  0  0]\n",
      " [22 18  0  0]\n",
      " [12  1  0  0]\n",
      " [22 44  0  0]\n",
      " [23 48 25  9]], shape=(10, 4), dtype=int32)\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001CBA0A7C3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.7000\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.2581 - accuracy: 0.0000e+00WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
      " 10/100 [==>...........................] - 0s 2ms/step - loss: nan - accuracy: 0.4000   \n",
      "1/1 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.5000\n",
      "Accuracy: 50.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_17/embeddings:0' shape=(50, 8) dtype=float32, numpy=\n",
       " array([[        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [-0.00975901,  0.03659991,  0.00138086, -0.01770953, -0.039822  ,\n",
       "          0.04544142,  0.01020611, -0.01391403],\n",
       "        [ 0.01775413,  0.01328962,  0.0058982 ,  0.03776171,  0.00299578,\n",
       "          0.01581695,  0.00724781, -0.00015415],\n",
       "        [-0.01844171, -0.03373764, -0.00300369, -0.03986211, -0.03833828,\n",
       "         -0.04944567,  0.01866418,  0.02923513],\n",
       "        [ 0.04291641,  0.04631516,  0.03798593,  0.0172174 , -0.02843343,\n",
       "         -0.00742146,  0.03445714,  0.00929978],\n",
       "        [-0.04071433,  0.03870375, -0.02513969,  0.02151484,  0.03394372,\n",
       "         -0.04691219, -0.01671691, -0.04387248],\n",
       "        [-0.01156149,  0.0364265 , -0.00733619, -0.04134567, -0.01369563,\n",
       "          0.04639937, -0.04566085, -0.02085301],\n",
       "        [ 0.04047516,  0.04668358,  0.003891  ,  0.00509514,  0.02008796,\n",
       "         -0.02090182, -0.0427825 ,  0.02153407],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.03782697, -0.0017219 , -0.04975605,  0.02061632,  0.02883003,\n",
       "         -0.03720337, -0.02877361, -0.0069306 ],\n",
       "        [ 0.03980904,  0.04135272,  0.00122545,  0.03990242, -0.00234363,\n",
       "          0.01607542,  0.04572728, -0.02566763],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.04946014, -0.02019533,  0.03436236, -0.03727543, -0.04652039,\n",
       "         -0.02462533, -0.03632166, -0.03658805],\n",
       "        [ 0.0276706 ,  0.00546869, -0.0124855 , -0.02440253, -0.02182878,\n",
       "          0.03582728, -0.04252888, -0.02420318],\n",
       "        [ 0.02020608,  0.00029825,  0.0302472 , -0.01848049, -0.00205618,\n",
       "          0.02408336,  0.02545125,  0.03298737],\n",
       "        [ 0.01179717, -0.00858293,  0.04148183, -0.02324073,  0.03812796,\n",
       "         -0.03896784, -0.03438463, -0.00016643],\n",
       "        [-0.01953136, -0.01547425,  0.04442401, -0.02343452,  0.00418974,\n",
       "         -0.00813685,  0.0277485 , -0.01576741],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.02146626,  0.03109106, -0.01728303, -0.04307656, -0.035839  ,\n",
       "         -0.03544005,  0.01152446, -0.02332188],\n",
       "        [-0.01155155,  0.00136188,  0.01200251,  0.00603582, -0.00843923,\n",
       "         -0.04483966,  0.04693872,  0.01954519],\n",
       "        [ 0.03620497, -0.02034986,  0.02231428, -0.00032481,  0.00570524,\n",
       "          0.03927625, -0.00037509, -0.02273352],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.04716755,  0.03502897,  0.03082686, -0.00023739,  0.01695824,\n",
       "         -0.03904052, -0.00630521, -0.03182213],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.002187  , -0.01345016, -0.04438818,  0.01576352,  0.02646789,\n",
       "          0.02480861, -0.03986635, -0.01459976],\n",
       "        [ 0.02532265, -0.03598417, -0.03274157, -0.00854626, -0.04574872,\n",
       "          0.03605882, -0.01445057, -0.04645776],\n",
       "        [ 0.02611038, -0.01591625, -0.00918411,  0.01381332, -0.00436018,\n",
       "         -0.03328384,  0.02709228, -0.01785083],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.01017617,  0.01410628,  0.03009539,  0.04314638, -0.00557209,\n",
       "         -0.00592601, -0.00345282, -0.00290493],\n",
       "        [ 0.04931689,  0.01904371,  0.04718309, -0.0442541 ,  0.01666266,\n",
       "         -0.00078101,  0.01209529,  0.03195158],\n",
       "        [ 0.03448211, -0.04817194, -0.01050295,  0.01637404,  0.01347469,\n",
       "         -0.02751305,  0.00421081, -0.01913048],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.02468515,  0.00722625, -0.02238538, -0.00828169, -0.00602067,\n",
       "          0.04431225,  0.02025643, -0.02914353],\n",
       "        [-0.03647811,  0.01678688,  0.03706578, -0.0229485 , -0.02163556,\n",
       "          0.00679746,  0.04671637, -0.04132383],\n",
       "        [-0.01681963,  0.00643973,  0.00231494, -0.0085515 , -0.01096501,\n",
       "          0.01260358,  0.00792712,  0.04608435],\n",
       "        [ 0.04641397,  0.04173868,  0.02532255,  0.03877283,  0.01596022,\n",
       "          0.02034733, -0.02932214,  0.00127529],\n",
       "        [-0.0498521 ,  0.04221569,  0.04314958,  0.02528257, -0.00877853,\n",
       "          0.04414039, -0.04633992,  0.04517056],\n",
       "        [ 0.02098055, -0.04224701,  0.02418939,  0.00218619,  0.03550447,\n",
       "          0.00763392, -0.0134784 ,  0.0137905 ],\n",
       "        [-0.02623699, -0.00121796, -0.0398563 , -0.03032837, -0.01409156,\n",
       "         -0.04834944, -0.02476116,  0.04732858],\n",
       "        [ 0.01848462,  0.028891  ,  0.01511308,  0.00748235, -0.02530018,\n",
       "         -0.01083429, -0.01195506, -0.01311476],\n",
       "        [-0.03230518, -0.01642685,  0.01552737, -0.04791957,  0.04564712,\n",
       "         -0.04908197, -0.03159361, -0.04408711],\n",
       "        [ 0.03465236,  0.04805953, -0.02040694,  0.01867339, -0.04724878,\n",
       "          0.03226608, -0.01748602,  0.02170089],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.04965898, -0.04992994, -0.04927751,  0.04004364, -0.03072879,\n",
       "         -0.03149476,  0.00422034,  0.04929337],\n",
       "        [ 0.02746587,  0.00867987,  0.04691346,  0.01143706, -0.03227472,\n",
       "          0.01073767,  0.00972063,  0.00221686],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.02794847,  0.04050467,  0.03757194, -0.01705294, -0.03954317,\n",
       "         -0.01022289,  0.01063862, -0.02895161]], dtype=float32)>,\n",
       " <tf.Variable 'dense_17/kernel:0' shape=(32, 1) dtype=float32, numpy=\n",
       " array([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], dtype=float32)>,\n",
       " <tf.Variable 'dense_17/bias:0' shape=(1,) dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = tf.convert_to_tensor(np.array([1,1,1,1,1,0,0,0,0,0]))\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "padded_docs = tf.convert_to_tensor(padded_docs)\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='mse', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=1, verbose=1, steps_per_epoch = 100)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "model.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44f43035",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-12a22a2b52d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54db9ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1160f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3adbd6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ee945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 25,  0,  0],\n",
       "       [ 1, 44,  0,  0],\n",
       "       [29, 18,  0,  0],\n",
       "       [23, 44,  0,  0],\n",
       "       [45,  0,  0,  0],\n",
       "       [44,  0,  0,  0],\n",
       "       [22, 18,  0,  0],\n",
       "       [12,  1,  0,  0],\n",
       "       [22, 44,  0,  0],\n",
       "       [23, 48, 25,  9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb399c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79a4d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_7/embeddings:0' shape=(50, 8) dtype=float32, numpy=\n",
       " array([[        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [-0.01492083, -0.02976065, -0.01549443,  0.02936831, -0.00519624,\n",
       "          0.02345714, -0.00900381,  0.02160739],\n",
       "        [-0.01563947,  0.03499044,  0.04195911, -0.00820928,  0.01859761,\n",
       "          0.03502626, -0.0036392 , -0.04014714],\n",
       "        [ 0.0149253 ,  0.04101967, -0.03607244,  0.04328681, -0.01374529,\n",
       "          0.03189918, -0.01114007,  0.03640575],\n",
       "        [-0.00171951,  0.02650141,  0.03808595, -0.00699276,  0.02841487,\n",
       "          0.0461097 ,  0.01732432, -0.03013566],\n",
       "        [ 0.01084407,  0.00994591,  0.02445711,  0.0182477 , -0.03799838,\n",
       "         -0.00964101, -0.00401131, -0.00267639],\n",
       "        [ 0.01701654,  0.02725198, -0.00345892, -0.02969965,  0.0241455 ,\n",
       "          0.04750972, -0.03735339, -0.04808072],\n",
       "        [-0.01602503, -0.03827728,  0.03890152, -0.04977311,  0.02086605,\n",
       "         -0.04877513, -0.03628761,  0.03699613],\n",
       "        [ 0.02730309, -0.00498873, -0.02484289,  0.04427094, -0.01820049,\n",
       "         -0.02160077,  0.04893991, -0.0118364 ],\n",
       "        [ 0.02314508,  0.01344384, -0.04378508, -0.04884679,  0.00706965,\n",
       "          0.04277522, -0.02824948, -0.00900946],\n",
       "        [ 0.04280851, -0.02075508, -0.04471169, -0.00186379, -0.02124456,\n",
       "          0.03003253,  0.0153104 ,  0.04483509],\n",
       "        [-0.02578797,  0.0012396 ,  0.0428657 ,  0.0413432 ,  0.00334089,\n",
       "          0.02281393, -0.00159117,  0.01548227],\n",
       "        [-0.03530698,  0.04142955,  0.03819979,  0.01228844,  0.01282846,\n",
       "          0.02767457, -0.03017604, -0.04459661],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.04004825,  0.01220043, -0.02437711,  0.02108691, -0.03259562,\n",
       "         -0.02402639,  0.0492649 , -0.03510723],\n",
       "        [ 0.00913223,  0.02576098, -0.01832218, -0.04090074, -0.02703319,\n",
       "          0.0297201 , -0.03561305, -0.03501312],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.0373907 ,  0.04750552,  0.02005212, -0.005538  , -0.0144232 ,\n",
       "          0.03006966, -0.01528049, -0.00053847],\n",
       "        [-0.02287502,  0.04084836,  0.03336406,  0.03832087, -0.0340681 ,\n",
       "         -0.03551905,  0.03360936,  0.01959285],\n",
       "        [ 0.02012372, -0.022856  ,  0.0133039 ,  0.03669969, -0.02598556,\n",
       "         -0.01083591, -0.03801621,  0.01977536],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [-0.00111395, -0.03434458, -0.0146892 , -0.04835441, -0.04213538,\n",
       "         -0.00976796, -0.04786757,  0.04890836],\n",
       "        [ 0.03058938,  0.03388032,  0.03072545, -0.04943017,  0.036594  ,\n",
       "         -0.02216457,  0.02218447,  0.03533304],\n",
       "        [-0.00848784,  0.03052444,  0.0019227 , -0.01517782, -0.03088964,\n",
       "         -0.03112829,  0.03015217,  0.04290387],\n",
       "        [ 0.02322413,  0.01849366, -0.01265336,  0.02557946,  0.00031818,\n",
       "         -0.02227459,  0.01203178,  0.01061027],\n",
       "        [-0.00757455,  0.03070804, -0.04336528,  0.01425772,  0.02552934,\n",
       "          0.00395774,  0.00108527, -0.04527203],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.00482636,  0.0247972 ,  0.0384875 ,  0.01440736, -0.00361142,\n",
       "          0.00386692,  0.0114597 ,  0.04578341],\n",
       "        [ 0.01340425, -0.01445693,  0.00869745,  0.00898732,  0.02330983,\n",
       "         -0.03504771, -0.03195572,  0.02502133],\n",
       "        [-0.00788905, -0.01574119, -0.00569023, -0.04995361, -0.01931515,\n",
       "         -0.04038993, -0.01577264,  0.00950966],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.0490755 , -0.00960283, -0.00771695,  0.01472281,  0.04238688,\n",
       "          0.0173727 ,  0.01387395,  0.02104295],\n",
       "        [-0.0430054 ,  0.0083288 , -0.04804656,  0.04519386, -0.01395758,\n",
       "          0.00135771, -0.04910875, -0.00265039],\n",
       "        [-0.01110735, -0.04456115, -0.01607171,  0.03047117, -0.04641496,\n",
       "          0.02758049, -0.01958169, -0.00586386],\n",
       "        [-0.03555721,  0.00854256, -0.00362496, -0.02570852, -0.04992905,\n",
       "          0.03373976, -0.02692561, -0.02468565],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.0414525 ,  0.00335147,  0.00067049,  0.02964444,  0.03337573,\n",
       "          0.01699761,  0.00275545,  0.00927992],\n",
       "        [ 0.02657079,  0.03715793,  0.03361073, -0.03583218, -0.0009892 ,\n",
       "         -0.01676713, -0.02567255,  0.01905341],\n",
       "        [ 0.04940393,  0.0360962 , -0.0355188 , -0.0242722 ,  0.04805428,\n",
       "         -0.02914571,  0.02839072,  0.00803803],\n",
       "        [-0.02879367,  0.04756993,  0.02733109, -0.04597671,  0.04468552,\n",
       "          0.03759744,  0.01485089, -0.00482269],\n",
       "        [ 0.0316003 ,  0.00347806, -0.01872148,  0.02204499, -0.00541921,\n",
       "         -0.0355489 , -0.03459451,  0.03123571],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan],\n",
       "        [ 0.00178249,  0.03316573, -0.04171998, -0.02496818,  0.0181313 ,\n",
       "         -0.02937049,  0.04533262, -0.02767861],\n",
       "        [ 0.0165428 , -0.04721848, -0.04426857,  0.02658567, -0.01011957,\n",
       "          0.045661  , -0.03137176,  0.01464805],\n",
       "        [ 0.03009478, -0.03155614,  0.02667919,  0.04536121,  0.04290236,\n",
       "          0.03594719, -0.00956105, -0.01581879],\n",
       "        [        nan,         nan,         nan,         nan,         nan,\n",
       "                 nan,         nan,         nan]], dtype=float32)>,\n",
       " <tf.Variable 'dense_7/kernel:0' shape=(32, 1) dtype=float32, numpy=\n",
       " array([[        nan],\n",
       "        [        nan],\n",
       "        [        nan],\n",
       "        [        nan],\n",
       "        [        nan],\n",
       "        [-0.14509112],\n",
       "        [        nan],\n",
       "        [-0.38114467],\n",
       "        [ 0.18396467],\n",
       "        [ 0.27180493],\n",
       "        [ 0.4118787 ],\n",
       "        [-0.05150861],\n",
       "        [        nan],\n",
       "        [        nan],\n",
       "        [ 0.18558604],\n",
       "        [-0.14348447],\n",
       "        [-0.1913433 ],\n",
       "        [ 0.23692876],\n",
       "        [ 0.02913056],\n",
       "        [ 0.3181948 ],\n",
       "        [ 0.30907938],\n",
       "        [ 0.33840153],\n",
       "        [ 0.20139259],\n",
       "        [-0.30555528],\n",
       "        [-0.07768404],\n",
       "        [ 0.21723855],\n",
       "        [ 0.31161255],\n",
       "        [-0.00350278],\n",
       "        [        nan],\n",
       "        [ 0.01801742],\n",
       "        [        nan],\n",
       "        [ 0.11255931]], dtype=float32)>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67dce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "tokenizer = text.Tokenizer(num_words=50)\n",
    "\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(docs)\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    # Turns text into into padded sequences.\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "sequences = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0df272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1cb9ca631f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e3758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
